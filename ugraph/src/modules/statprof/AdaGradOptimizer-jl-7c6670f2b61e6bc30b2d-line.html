<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
   "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <link rel="stylesheet" type="text/css" href="statprofiler.css">

    <title>StatProfilerHTML.jl: /home/arashyazdiha/.julia/v0.6/GradDescent/src/AdaGradOptimizer.jl</title>
  </head>
  <body>
<div class="report-header">
  <div class="backlink"><a href="index.html">Report index</a></div>
  <div class="report-title">StatProfilerHTML.jl report</div>
  <div class="generated-on">Generated on di 03 apr 2018 13:39:33 CEST</div>
</div>

    <div id="body-content">
      <table>
        <caption>File source code</caption>
        <tr>
          <th>Line</td>
          <th>Exclusive</th>
          <th>Inclusive</th>
          <th>Code</th>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-1"></a>1</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">mutable struct Adagrad &lt;: Optimizer
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-2"></a>2</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    opt_type::String
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-3"></a>3</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    t::Int64
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-4"></a>4</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    ϵ::Float64
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-5"></a>5</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    η::Float64
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-6"></a>6</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    G_t::Array{Float64}
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-7"></a>7</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">end
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-8"></a>8</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-9"></a>9</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">&quot;Construct Adagrad optimizer&quot;
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-10"></a>10</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">function Adagrad(; η::Float64=0.01, ϵ::Float64=1e-8)
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-11"></a>11</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    η &lt;= 0.0 &amp;&amp; error(&quot;η must be greater than 0&quot;)
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-12"></a>12</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    ϵ &lt;= 0.0 &amp;&amp; error(&quot;ϵ must be greater than 0&quot;)
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-13"></a>13</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-14"></a>14</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    Adagrad(&quot;Adagrad&quot;, 0, ϵ, η, zeros(1))
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-15"></a>15</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">end
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-16"></a>16</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-17"></a>17</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">params(opt::Adagrad) = &quot;ϵ=$(opt.ϵ), η=$(opt.η)&quot;
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-18"></a>18</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-19"></a>19</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">function update(opt::Adagrad, g_t::Array{Float64})
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-20"></a>20</td>
          <td></td>
          <td></td>
          <td>
            <div class="call-sites">
              <a name="S-home-arashyazdiha-julia-v0-6-GradDescent-src-AdaGradOptimizer-jl-update-20"></a>
              57 (0.59%) samples spent in update<br />
              6 (100.00%) (ex.),
              57 (100.00%) (incl.)
              when called from
              updateμ!
              <a href="trainutils-jl-24f2bf461fb71fb4e996-line.html#Ltrainutils-jl-24f2bf461fb71fb4e996-line.html-114">line 114</a><br />
            </div>
            <span class="code">    # resize squares of gradients
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-21"></a>21</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    if opt.t == 0
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-22"></a>22</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">        opt.G_t = zeros(Float64, length(g_t))
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-23"></a>23</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    end
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-24"></a>24</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-25"></a>25</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-26"></a>26</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    # update timestep
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-27"></a>27</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    opt.t += 1
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-28"></a>28</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-29"></a>29</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">    # accumulate squares of gradients
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-30"></a>30</td>
          <td>2 (0.02%)</td>
          <td>23 (0.24%)</td>
          <td>
            <div class="callees">
              5 (21.74%)
              samples spent calling
              <a href="arraymath-jl-b564666165cf37ea0005-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-arraymath-jl-37">+</a><br />
              4 (17.39%)
              samples spent calling
              <a href="broadcast-jl-ef111e8220ee56a27253-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-broadcast-jl-broadcast-455">broadcast</a><br />
              2 (8.70%)
              samples spent calling
              <a href="array-jl-c31e5f0854745d9d2638-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-array-jl-setindex-629">setindex!</a><br />
              10 (43.48%)
              samples spent calling
              <a href="array-jl-c31e5f0854745d9d2638-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-array-jl-getindex-568">getindex</a><br />
            </div>
            <span class="code">    opt.G_t[:] += (g_t .^ 2.0)
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-31"></a>31</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-32"></a>32</td>
          <td>4 (0.04%)</td>
          <td>4 (0.04%)</td>
          <td>
            <span class="code">    δ = zeros(Float64, length(g_t))
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-33"></a>33</td>
          <td></td>
          <td>26 (0.27%)</td>
          <td>
            <div class="callees">
              10 (38.46%)
              samples spent calling
              <a href="broadcast-jl-ef111e8220ee56a27253-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-broadcast-jl-broadcast-455">broadcast</a><br />
              14 (53.85%)
              samples spent calling
              <a href="array-jl-c31e5f0854745d9d2638-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-array-jl-getindex-568">getindex</a><br />
              2 (7.69%)
              samples spent calling
              <a href="array-jl-c31e5f0854745d9d2638-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-array-jl-setindex-629">setindex!</a><br />
            </div>
            <span class="code">    δ[:] = opt.η ./ (sqrt.(opt.G_t[:] .+ opt.ϵ)) .* g_t[:]
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-34"></a>34</td>
          <td></td>
          <td>4 (0.04%)</td>
          <td>
            <div class="callees">
              4 (100.00%)
              samples spent calling
              <a href="array-jl-c31e5f0854745d9d2638-line.html#S-home-arashyazdiha-Documents-Programs-julia-d386e40c17-bin-share-julia-base-array-jl-getindex-568">getindex</a><br />
            </div>
            <span class="code">    return δ[:]
</span>
          </td>
        </tr>

        <tr>
          <td><a name="LAdaGradOptimizer-jl-7c6670f2b61e6bc30b2d-line.html-35"></a>35</td>
          <td></td>
          <td></td>
          <td>
            <span class="code">end
</span>
          </td>
        </tr>
      </table>
    </div>
  </body>
</html>
