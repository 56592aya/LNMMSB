#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{tkz-graph}  
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{fit}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style 
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.2cm
\topmargin 1.5cm
\rightmargin 1.2cm
\bottommargin 1.2cm
\headheight 1.2cm
\headsep 1.2cm
\footskip 1.2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section*
Rough Chapter 1 and 2
\end_layout

\begin_layout Part
Introduction
\end_layout

\begin_layout Standard
Observational social data exhibit many complexities and researchers tends
 to exploit the underlying information on a much simpler and more intuitive
 manner by detecting patterns that can explain what we actually observe.
 The patterns are not directly observable, and hence, researchers use various
 unsupervised methods to capture the simpler latent structure, implicit
 in the data.
 Availability of such patterns can encourage researchers to ask new questions,
 or refine their common theories with finer grained explanations.
 In this work we develop models that can find these patterns in social network
 data, and use them to better understand behavior by refining some of the
 well known problems, such as identifying influentials, contagion signals,
 and various co-evolutions in large network data sets.
 Moreover using Bayesian techniques in modeling and more recent statistical
 inference methods, alongside with ubiquitous computational advantages,
 we achieve scalability and efficiency without much sacrificing accuracy.
 We take a model-based approach to formulate social structure and behavior
 in networks by offering flexible assumption based on network theory and
 scalability in inference.
\end_layout

\begin_layout Section
Community Structure
\end_layout

\begin_layout Standard
Ample amount of research has recognized a prevalent feature that networks
 exert meaningful smaller groups
\begin_inset CommandInset citation
LatexCommand cite
key "girvan2002community,fortunato2010community,newman2006modularity"

\end_inset

.
 A common phenomenon observed in many social networks including multilevel
 relationships signifies shared community memberships, hence we define communiti
es as groups of individuals yielding a better understanding of the network
 connections, where individual vertices can belong to multiple clusters.
 Many different methods regarding community detection have been developed
 among which address the problem in terms of algorithmic
\begin_inset CommandInset citation
LatexCommand cite
key "newman2004fast,palla2005uncovering,zubcsek2014information"

\end_inset

 or model-based(probabilistic) approaches
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed,handcock2007model,hoff2002latent"

\end_inset

.
 Our model based approach can yield overlapping communities that provides
 us with a tool to explain the real patterns of connections more accurately,
 and enables us to assess the meaning of these communities, role of individuals
 in each domain, and behavioral patterns.
 To explain the usefulness of these tools, we use several simulated synthetic
 and actual real world social networks, and elaborate further on how they
 provide us both with better answers about existing problems, as well as
 help us refine and ask better questions.
 Community detection can be seen as a way of reducing the dimension of relationa
l data.
\end_layout

\begin_layout Standard
For proposing a model-based approach, we have to provide theoretical support
 on how networks form.
 The merits are that including our knowledge on structure of networks can
 provide us with better tools to understand processes that can happen in
 parallel at network level.
 But even without these processes we may be able to learn more about individuals
 and their impact on forming connections or facilitating message traverse
 by evaluating their sub-network level characteristics such as bridgeness
 or centrality.
 Furthermore with availability of behavioral data, we can gain a better
 meaning of the detected communities, and improve both our community detection
 and prediction of signal penetration.
\end_layout

\begin_layout Section
Outline
\end_layout

\begin_layout Standard
To develop our general theory and to build a model of network structure,
 we follow the argument of assortative mixing and homophily
\begin_inset CommandInset citation
LatexCommand cite
key "lazarsfeld1954friendship,mcpherson2001birds,newman2002assortative,newman2003mixing"

\end_inset

 suggesting individuals in a social network tend to communicate with rather
 similar people.
 This phenomenon leads to patterns of structure in networks where we observe
 denser groups of alike individuals that have fewer connections to the rest
 of the network.
 Homophily does not inhibit individuals in a dense group to connect to nodes
 in other clusters.
 This is one of the main considerations that distinguishes more recent technique
s in community detection from earlier clustering methods that assume blocks
 of rather disconnected individuals.
 Our research builds on mixed membership models that add another level of
 hierarchy on top of mixture assignments to groups.
\end_layout

\begin_layout Standard
In chapter 2 we propose our first model of network structure that incorporates
 the notion of overlapping communities.
 We extend previous models by allowing correlated communities, as some may
 induce friendships while others inhibit such connections.
 In this paper we account for such co-occurrence via introducing correlations
 among the community memberships of individuals.
 We depart from conventional mixed membership models by introducing these
 correlations that can explain more about the natural observed networks.
 We argue that accounting for such structure both complies with the assortative
 mixing theory and performs better compared to the conventional mixed-membership
 stochastic block model
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed"

\end_inset

.
 Moreover, we provide a more comprehensive expression of connections that
 allow for directedness of edges and can well be applied for undirected
 networks.
 Scalability and efficiency of inferential techniques in such models that
 introduce even more link-level and individual-level parameters is a common
 concern that has been extensively treated in machine learning literature.
 As adding direction and correlational structure among community memberships
 introduce more parameters, we further develop the appropriate methodology
 to make inference about our parameters in a fast and scalable manner without
 sacrificing much of accuracy.
 To test the success of our model, we use several synthetic and real world
 observational social networks.
 Our empirical study of large real world social network allows us to gain
 further insights on the added values of detecting overlapping communities.
 These insights inform us of detailed measures of importance at the community
 level, rather than on the network level.
 In this case, connectors may not be the most influential people, but connectors
 in specific communities, or bridges between domains, can be better messengers
 or targets for seeding through networks.
 Such roles may appear as domain specific knowledge brokers, or sub-network
 level connectors.
 We use graphical models, specifically directed acyclic graphs(DAG) to visualize
 the problems at hand following the notations from 
\begin_inset CommandInset citation
LatexCommand cite
key "pearl2009causality,morgan2014counterfactuals"

\end_inset

.
 DAGs provide tools both from graph theory and statistics to formulate generativ
e models and further assess causal implication of the model.
 The figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Graphical-representation1"

\end_inset

 presents our a priori knowledge of the network structure, where shaded
 
\begin_inset Formula $y_{ij}$
\end_inset

 represents the observed connection between actor 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

, and the unshaded 
\begin_inset Formula $z_{i}$
\end_inset

, and 
\begin_inset Formula $z_{j}$
\end_inset

 represent their unknown latent preferences for forming connections.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
usetikzlibrary{arrows}
\end_layout

\begin_layout Plain Layout


\backslash
tikzset{     
\end_layout

\begin_layout Plain Layout

	vertex/.style={circle,draw,minimum size=1.5em},
\end_layout

\begin_layout Plain Layout

    edge/.style={->,> = latex'} 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout

% vertices 
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex, fill={rgb:black,1;white,2}] (y) at (6,0) {$y_{ij}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zi) at (4,4) {$z_{i}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zj) at (8,4) {$z_j$};
\end_layout

\begin_layout Plain Layout


\backslash
node [draw=red, fit= (zi) (zj),  inner sep=1.00cm, dashed, ultra thick, fill=red!
10, fill opacity=0.2] {};
\end_layout

\begin_layout Plain Layout

%edges 
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi) to [out=300,in=110] (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj) to [out=240,in=70] (y);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Graphical representation of network structure
\begin_inset CommandInset label
LatexCommand label
name "fig:Graphical-representation1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Although, social data alone can provide many useful insights about positions
 of individuals and potential sub-network communities, behavioral data can
 prove beneficial in improving the community detection and add more meaning
 to the context of different overlapping domains.
 Bulk of research in assessing behavior in social network overlooks the
 potential synergy of collective account of social and behavioral modeling.
 In chapter 3 we intend to gain some insights about networked behavior and
 community structure using the community detection method we developed in
 chapter 2.
 We introduce a new approach in latent variable models for directed networks,
 where individuals make decisions according to both their preferences for
 link formation and behavior, where these preferences are unobserved to
 the researcher.
 In other words we jointly model both behavior and links by assuming actions
 and connections between individuals can come from a potentially shared
 latent space.
 Keeping in mind that behavioral responses and network structure may not
 be completely separate, the results of this study can shed light on more
 important problems in marketing, sociology,health economics, etc.
 such as study of social influence and identifying opinion leaders.
 Our approach combines two mixed membership models, with one developed in
 chapter 2, to model both choices/actions of actors and links between individual
s.
 Again, inference is possible by exploiting an efficient and scalable inferentia
l technique that we use in chapter 2.
 As a result we can delve more into the implications of this approach and
 its added value to the model of chapter 2 in improving the prediction of
 of signal penetration and evaluating the enriched community level patterns
 with more domain specific and rather counter-intuitive interpretation.
 Our empirical investigation can prove that positioning of individuals regarding
 their centrality or bridgeness, can act very differently depending on the
 domain specific reach.
 For example, one with high centrality at the network level, may appear
 not to be an opinion leader in one domain and/or others.
 This can have major significance for marketers, who are in charge of developing
 strategies in designing WOM campaign, targeting influentials, or segmenting
 consumers for better reach.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Graphical-representation-of2"

\end_inset

 visualizes the formation of both links and behavior in a DAG.
 The network structure is shared with the model developed in chapter 2,
 and 
\begin_inset Formula $z_{1}$
\end_inset

and 
\begin_inset Formula $z_{2}$
\end_inset

 represent the same 
\begin_inset Formula $z$
\end_inset

 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Graphical-representation1"

\end_inset

, but we differentiate between the part of the latent space that is responsible
 for link structure and adoption behavior.
 We further assume that there is a latent space purely responsible for adoption
 behavior portrayed as 
\begin_inset Formula $z_{3}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
usetikzlibrary{arrows}
\end_layout

\begin_layout Plain Layout


\backslash
tikzset{     
\end_layout

\begin_layout Plain Layout

	vertex/.style={circle,draw,minimum size=1.5em},
\end_layout

\begin_layout Plain Layout

    edge/.style={->,> = latex'} 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout

% vertices 
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex,fill={rgb:black,1;white,2}] (y) at (6,-2) {$y_{ij}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zi1) at (4,4) {$z_{i1}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zi2) at (2,4) {$z_{i2}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zi3) at (0,4) {$z_{i3}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zj1) at (8,4) {$z_{j1}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zj2) at (10,4) {$z_{j2}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zj3) at (12,4) {$z_{j3}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex,fill={rgb:black,1;white,2}] (Ai) at (3,0) {$A_i$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex,fill={rgb:black,1;white,2}] (Aj) at (9,0) {$A_j$};
\end_layout

\begin_layout Plain Layout


\backslash
node[draw=red, fit= (zi1) (zj1) (zi2) (zj2),  inner sep=.60cm, dashed, ultra
 thick, fill=red!10, fill opacity=0.2] {};
\end_layout

\begin_layout Plain Layout


\backslash
node[draw=blue, fit= (zi1) (zj1) (zi2) (zj2) (zi3) (zj3),  inner sep=1.20cm,
 dashed, ultra thick, fill=blue!10, fill opacity=0.2] {};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (2.4,4.3) {Link Preferences};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (-.5,4.8) {Latent Space};
\end_layout

\begin_layout Plain Layout

%edges 
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi1) to [out=300,in=110] (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj1) to [out=240,in=70] (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi2) -- (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj2) -- (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi2) to [out=270,in=110] (Ai);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj2) to [out=270,in=70] (Aj);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi3) to [out=270,in=180] (Ai);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj3) to [out=270,in=0] (Aj);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Graphical representation of network structure and behavior
\begin_inset CommandInset label
LatexCommand label
name "fig:Graphical-representation-of2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Observational data on social networks suggest that individuals make decisions
 that cluster both in network circles and in time
\begin_inset CommandInset citation
LatexCommand cite
key "aral2009distinguishing"

\end_inset

.
 A major hurdle to identify the causal effect of individuals on each other
 is imposed by the latent homophily, where similar people with analogous
 preferences may be responsible for their homogeneous behavior.
 As can be seen in the graph in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Graphical-representation-of2"

\end_inset

, identifying influence from homophily may seem impossible non-parametrically,
 as we cannot control for any of the unobserved traits.
 Our approach tries to alleviate this problem by controlling for the communities
 as a proper proxy for the 
\begin_inset CommandInset citation
LatexCommand cite
key "shalizi2011homophily"

\end_inset

.
 
\end_layout

\begin_layout Standard
It is not clear to what extent, commonly known measures such as degree centralit
y, betweenness, prestige, etc are able to discover influentials, a concept
 that can depend on how one defines the communities at hand
\begin_inset CommandInset citation
LatexCommand cite
key "jackson2010social"

\end_inset

.
 Chen et al (2017) suggest that incorporating connection specific characteristic
s in a multi-graph network can yield much improved prediction of diffusion
 when the seeding strategies are adjusted accordingly rather than exploiting
 traditional centrality measures
\begin_inset CommandInset citation
LatexCommand cite
key "chen2017uncovering"

\end_inset

.
\end_layout

\begin_layout Standard
On the other hand many evidence from real world networks show signs of evolution
 of networks in time
\begin_inset CommandInset citation
LatexCommand cite
key "brueckner2007workings,jackson2002evolution,snijders2007modeling"

\end_inset

.
 This means that at certain times in the network some ties are formed and
 some are severed.
 Brot et al(2016) argue that this happens when observing bursts in connection
 that underlies the change in homophily
\begin_inset CommandInset citation
LatexCommand cite
key "brot2016evolution"

\end_inset

.
 Hence trying to explain these rather stochastic changes requires a better
 understanding of homophily and formation of networks.
\end_layout

\begin_layout Standard
Chapter 4 builds on the findings of chapter 2 and 3 and extends these models
 to a dynamic setting of networks, where we aim for finding evidence of
 contagion driven homophily.
 Current studies regarding disentanglement of influence-driven and homophily
 driven behavior assume that these two phenomena act separately on decision
 making of individuals.
 We argue that preferences of individuals mapped to a latent space can indeed
 change according to their adoption behavior.
 A dynamic networked data, enables us to monitor the preference changes
 of individuals when they adopt or refuse to adopt a behavior from their
 peers.
 While Chapter 2 gives an insight on how to model both link formation and
 decisions made by individuals, dynamic nature of contagion, requires us
 to have a model that allows the latent structure to develop through time.
 Still controlling for latent proxies we can monitor the bursts of link
 formation and severance and find evidence that adopting similar behaviors
 among individuals depending on the intensity of their relationship can
 lead to either more or less homophilous bondings.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
usetikzlibrary{arrows}
\end_layout

\begin_layout Plain Layout


\backslash
tikzset{     
\end_layout

\begin_layout Plain Layout

	vertex/.style={circle,draw,minimum size=1.5em},
\end_layout

\begin_layout Plain Layout

    edge/.style={->,> = latex'} 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout

% vertices 
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex,fill={rgb:black,1;white,2}] (y) at (6,-2) {$y_{ijt}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zi1) at (4,4) {$z_{i1t}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zi2) at (2,4) {$z_{i2t}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zi3) at (0,4) {$z_{i3t}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zj1) at (8,4) {$z_{j1t}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zj2) at (10,4) {$z_{j2t}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zj3) at (12,4) {$z_{j3t}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex,fill={rgb:black,1;white,2}] (Ai) at (3,0) {$A_{it}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex,fill={rgb:black,1;white,2}] (Aj) at (9,0) {$A_{jt}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[draw=red, fit= (zi1) (zj1) (zi2) (zj2),  inner sep=.60cm, dashed, ultra
 thick, fill=red!10, fill opacity=0.2] {};
\end_layout

\begin_layout Plain Layout


\backslash
node[draw=blue, fit= (zi1) (zj1) (zi2) (zj2) (zi3) (zj3),  inner sep=1.20cm,
 dashed, ultra thick, fill=blue!10, fill opacity=0.2] {};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (2.4,4.3) {Link Preferences};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (-.5,4.8) {Latent Space};
\end_layout

\begin_layout Plain Layout


\backslash
node[draw=black, fit= (zi1) (zj1) (zi2) (zj2) (zi3) (zj3) (y) (Ai) (Aj),
  inner sep=1.40cm, dashed, ultra thick, fill=black!10, fill opacity=0.2]
 {};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (13,-3.5) {$t=1..T$};
\end_layout

\begin_layout Plain Layout

%edges 
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi1) to [out=300,in=110] (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj1) to [out=240,in=70] (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi2) -- (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj2) -- (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi2) to [out=270,in=110] (Ai);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj2) to [out=270,in=70] (Aj);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zi3) to [out=270,in=180] (Ai);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zj3) to [out=270,in=0] (Aj);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Dynamic representation of co-evolution
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:summary-of-chapters"

\end_inset

 provides a summary of chapters.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="left" valignment="top" width="4cm">
<column alignment="left" valignment="top" width="4cm">
<column alignment="left" valignment="top" width="4cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Chapter 2
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Chapter 3
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Chapter 4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Topic
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Modeling network structure via correlated community membership
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
 Insights from networked behavior via community structure
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Contagion driven homophily in large dynamic networks
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Goal
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
overlapping community detection 
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
finding domain specific individual 
\emph default
experts 
\emph on
at community level based on positioning(brokerage role, community centrality)
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
exploring meaning of community detection on network data
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
exploiting behavioral data in better detecting communities 
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
assessing improved prediction of signal penetration rate
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
refining definitions of opinion leaders
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Monitoring changes in homophilous structure as a function of behavior
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Logistic Normal Mixed-Membership-Stochastic-Blockmodel
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Joint LDA and LNMMSB
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
dynamic Joint LDA and LNMMSB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Inference
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Variational Inference
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
Stochastic Variational Inference
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Stochastic Variational Inference
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Stochastic Variational Inference
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Data
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
Synthetic small
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
Synthetic large
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
political blogs
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
bookmarking network
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
synthetic small
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
synthetic large
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
bookmarking network
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
synthetic small
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
synthetic large
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
bookmarking network
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Implications
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
scalable inference for large directed networks
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
accounting for potential correlation between community membership of individuals
\end_layout

\begin_layout Itemize

\size scriptsize
domain specific influentials may not be influentials in other domains or
 central in general
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
scalable inference for large networked behavior data
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
jointly modeling behavior and network structure
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
relevance of social data in predicting behavior
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Itemize

\size scriptsize
\emph on
dynamic evolution of social networks jointly modeled with behavioral data
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
evidence of behavior affecting network positions
\end_layout

\begin_layout Itemize

\size scriptsize
\emph on
using community level measures to redefine opinion leadership
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
summary of chapters
\begin_inset CommandInset label
LatexCommand label
name "tab:summary-of-chapters"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part
Chapter 2
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{abstract}
\end_layout

\begin_layout Plain Layout

It is widely known that individuals in a network form smaller communities,
 where each individual can belong to several groups.These groups namely communiti
es/clusters share the property that within group connections among the vertices
 of the graph are denser compared to the between group connections.
 Traditionally the problem of clustering networks included partitioning
 graphs into separate groups, whereas more recent works represent the embodiment
 of more natural processes such as shared community memberships.Although
 caution must be taken regarding how one defines communities, as the concept
 could vary dramatically depending on the given context.
 Another important aspect of these separate/shared communities should be
 encoded via their correlation, as some may induce friendships while others
 inhibit such connections.
 In this paper we account for such co-occurrence via introducing correlations
 among the community memberships of individuals via logistic normal prior.
 We argue that accounting for such correlations both complies with the assortati
ve mixing theory and performs better compared to the widely known mixed-membersh
ip stochastic block model.
 We further investigate the ability of our model to detect overlapping communiti
es in both synthetic and real world large scale networks, and use the results
 to highlight the superiority of community level measures in comparison
 to conventional network level centrality measures.
\end_layout

\begin_layout Plain Layout


\backslash
end{abstract}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Many real-life social and other types of networks of individuals exhibit
 smaller grouped structures
\begin_inset CommandInset citation
LatexCommand cite
key "bickel2009nonparametric,newman2006modularity"

\end_inset

.
 Some of these smaller groups are portrayed as almost disconnected sub-networks
\begin_inset CommandInset citation
LatexCommand cite
key "newman2004detecting,snijders1997estimation"

\end_inset

.
 More recently these smaller patterns are more accurately described by multiple
 interconnected networks instead of one 
\shape italic
homogeneous
\shape default
 network
\begin_inset CommandInset citation
LatexCommand cite
key "yang2012community,xie2013overlapping,lancichinetti2009detecting,airoldi2008mixed"

\end_inset

.
 While theoretically we still expect much predictive ability from the network
 measures studied till now
\begin_inset CommandInset citation
LatexCommand cite
key "goel2013predicting"

\end_inset

, empirically this impact will be blurred because the network is in fact
 a mixture of multiple networks, leaving the empirical assessment of network
 measures a shaky foundation.
 Identifying these constructs could still be valuable even in the absence
 of behavioral data.
 For example accounting for shared communities in a directed networks, can
 signal information about the identification and potential power of information
 brokers sitting at the edge of the groups
\begin_inset CommandInset citation
LatexCommand cite
key "gopalan2012scalable,nepusz2008fuzzy"

\end_inset

.
 For those individuals belonging to several communities, it is important
 to account for the size of their incoming connections given their outgoing
 links.
 This can shed light on their potential brokerage power on specific topic
 communities.
\end_layout

\begin_layout Standard
Latent variable models have opened a new pathway in deriving intuition from
 the underlying patterns in network structure.Braun and Bonfrer (2011) use
 a latent space approach to map individual latent traits to a Euclidean
 space.
 In latent space modeling, individual characteristics are represented as
 hidden structures that need to be estimated from the network data, where
 potentially infinite dimensional individual characteristics are mapped
 to a lower dimensional Euclidean space; and the similarity is measured
 by the distance between individual locations in this low dimensional space
\begin_inset CommandInset citation
LatexCommand cite
key "hoff2002latent,braun2011scalable,ansari2011modeling"

\end_inset

.
 Braun and Bonfrer (2011) argue that latent space approach could provide
 very different insights compared to geodesic distances,especially when
 evaluating the reach.
 More individuals could potentially and better semantically be reached through
 a small radius around a focal node in a low dimensional space rather than
 accommodating first and second degree connections of that focal node that
 may end up to be irrelevant.This again suggests how strategies can lead
 to different results when social data are applied differently through community
 detection and geodesic reach
\begin_inset CommandInset citation
LatexCommand cite
key "braun2011scalable"

\end_inset

.
 While latent (Euclidean) space models
\begin_inset CommandInset citation
LatexCommand cite
key "braun2011scalable,ansari2011modeling,handcock2007model,hoff2002latent"

\end_inset

 account for additional structure in the network formation process, they
 do not provide access to network measures for various sub-networks as they
 do not leave much for interpretation when representing the network in a
 Euclidean space.
\end_layout

\begin_layout Standard
We employ a model-based approach that allows us to define the network structure
 according to a set of hypotheses in line with the context under study and
 the theory underlying the social formation of friendships.
 We follow the argument of assortative mixing and homophily
\begin_inset CommandInset citation
LatexCommand cite
key "mcpherson2001birds,newman2002assortative,newman2003mixing"

\end_inset

 suggesting individuals in a social network tend to communicate with rather
 similar people.
 This phenomenon leads to patterns of structure in networks where we observe
 denser groups of alike individuals that have fewer connections to the rest
 of the network.
 Affinity of individuals and how to measure the likeness among them is condition
al on both context and the availability of information on the individual
 level.
 Aral et al(2009) employ 20 individual and network characteristics as a
 proxy for similarity between friends where the degree of closeness is measured
 by cosine distance.
 Using a dynamic matched sample estimation, they found further evidence
 that mobile application behavior could be partly be explained by homophily
 rather than mere social influence
\begin_inset CommandInset citation
LatexCommand cite
key "aral2009distinguishing"

\end_inset

.
 More recent works on detection of communities tend to account for overlapping
 structures that allow individual to belong to several communities
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed,gopalan2013efficient,yang2012community"

\end_inset

.Yang et al (2012) propose affiliation graph model that allows for detection
 of dense overlaps in the community network
\begin_inset CommandInset citation
LatexCommand cite
key "yang2012community"

\end_inset

.
 Airoldi et al (2008) suggest a mixed-membership-stochastic-blockmodel(MMSB)
 that allows individuals to belong to multiple groups by trying to estimate
 community membership strength
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed"

\end_inset

.
 We adopt the model of MMSB
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed,gopalan2013efficient"

\end_inset

 and extend it to allow for more flexible specification and scalable inference.Fo
r similar ideas of soft clustering and mixed membership in marketing refer
 to 
\begin_inset CommandInset citation
LatexCommand cite
key "varki2000modeling,varki2004augmented,jacobs2016model"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Another important type of detectable communities can arise not from the
 densely connected groups but rather dense patterns of connections.Yang et
 al (2014) argues that patterns of connections may also be an indicator
 of different communities when observing denser intra-cluster compared to
 inter-cluster connections
\begin_inset CommandInset citation
LatexCommand cite
key "yang2014detecting"

\end_inset

.
 not very relevant.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A common missing piece in many network studies in the field of marketing
 is that most measures are only evaluated in small scale networks or do
 not offer scalable methods to tackle the problem at hand.
 A related stream of diffusion research, studies the contagion phenomenon
 in new prescription drugs among a network of physicians
\begin_inset CommandInset citation
LatexCommand cite
key "iyengar2011opinion,van2007new,van2001medical"

\end_inset

.
 Other examples, with larger networks include 
\begin_inset CommandInset citation
LatexCommand cite
key "hinz2011seeding"

\end_inset

 that use a large network of mobile users, 
\begin_inset CommandInset citation
LatexCommand cite
key "goldenberg2009role"

\end_inset

 with a Korean social network,
\begin_inset CommandInset citation
LatexCommand cite
key "katona2011network"

\end_inset

 using a large European social network,
\begin_inset CommandInset citation
LatexCommand cite
key "aral2009distinguishing"

\end_inset

 that use a large IM network,
\begin_inset CommandInset citation
LatexCommand cite
key "trusov2010determining"

\end_inset

 that use both large simulation and field data.
 ...????.
 Although large networks have been in place but a lot of these face challenges
 in estimation when it comes to making inference about latent traits or
 individual-link level parameters.
 We try to address this problem by using a stochastic variational inference
 that does not sacrifice accuracy.Among studies that incorporate large scale
 networks are (
\begin_inset CommandInset citation
LatexCommand cite
key "braun2011scalable"

\end_inset

,?) .
\end_layout

\begin_layout Standard
When dealing with large scale data, little to no attention is given to the
 directedness of connections in social networks under study.
 Many social and relational data structures arise from the directed connections
 between the nodes, where the directed edge implies a connection from one
 node to the other that is not necessarily reciprocated.
 Examples of this behavior could be observed in networks such as twitter
 followership
\begin_inset CommandInset citation
LatexCommand cite
key "cataldi2010emerging,kwak2010twitter"

\end_inset

, co-authorship and citation networks
\begin_inset CommandInset citation
LatexCommand cite
key "liu2005co,kempe2003maximizing"

\end_inset

, and many more
\begin_inset CommandInset citation
LatexCommand cite
key "hummon1989connectivity"

\end_inset

.Although many treatments of network connections assume undirectedness
\begin_inset CommandInset citation
LatexCommand citet
key "braun2011scalable,gopalan2013efficient,palla2005uncovering"

\end_inset

, we argue that a lot of important features underlying the edges could be
 misjudged, lost, or lead to biased estimates.
 Direction can be interpreted as the main denominator of followership, where
 the same cannot be reciprocated.
 Additionally, when behavioral data is added, time and direction cannot
 be separated from the network.
 In other words contacting an individual via a mobile app, emailing colleagues
 in an organizational setting, liking a post of brand page or a friend's
 comments, following a celebrity or a friend in a microblog, cannot be easily
 treated as a mutual relationships;But time and the nature of communication
 dictates a specific orientation of a link from one person to another.
 Hence we allow for directed edges that can further signal more information
 about the clustering of the network.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Also put marketing in here
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Implications of finding overlapping communities can be manifold.
 To gain better understanding of diffusion of ideas(),products
\begin_inset CommandInset citation
LatexCommand cite
key "van2007new,aral2012identifying"

\end_inset

,medical innovation
\begin_inset CommandInset citation
LatexCommand cite
key "van2001medical,coleman1966medical"

\end_inset

, one has to be able to acknowledge different sources of contagion.
 Behaviors and decisions made by many individuals in observed networks tend
 to assimilate both in node space and in time
\begin_inset CommandInset citation
LatexCommand cite
key "aral2009distinguishing"

\end_inset

.
 However disentangling the underlying reasons can become infeasible due
 to endogenous network formation portrayed by latent homophily
\begin_inset CommandInset citation
LatexCommand cite
key "shalizi2011homophily"

\end_inset

.
 Conveniently addressing latent homophily and using a proxy estimation could
 improve the estimation of influence versus homophily
\begin_inset CommandInset citation
LatexCommand cite
key "shalizi2011homophily,shalizi2016controlling,davin2015essays"

\end_inset

.
 We attend to this problem furthermore in chapter 3 by jointly modeling
 both network structure and behavior and in chapter 4 by adding dynamics.Estimati
ng MMSB , or in fact any model that estimates the latent factors that drive
 link formation, also helps in solving the identification problem of disentangli
ng influence from homophily as suggested by 
\begin_inset CommandInset citation
LatexCommand cite
key "shalizi2011homophily,shalizi2016controlling"

\end_inset

.
 
\end_layout

\begin_layout Standard
Extensive studies of social contagion in marketing, have dealt with the
 partial role of structure in studying influence or disentangling causality
 from homophily
\begin_inset CommandInset citation
LatexCommand citep
key "aral2009distinguishing,aral2012identifying,goldenberg2009role,van2001medical"

\end_inset

.
 However sociometric data could prove to be useful.
 Goldenberg et al (2009), show that actors with higher degree centrality(hubs)
 can speed up contagion or lead to higher volume diffusion depending on
 being innovative or follower
\begin_inset CommandInset citation
LatexCommand cite
key "goldenberg2009role"

\end_inset

.
 Goel and Goldstein(2014) have shown that regardless of the causal nature
 of influence and possible selection biases in clustering of behaviors,
 social data can prove to be beneficial and complementary to behavioral
 data in terms of predicting the future actions
\begin_inset CommandInset citation
LatexCommand cite
key "goel2013predicting"

\end_inset

.
 Hence a good knowledge on the mechanics of structure formation in social
 networks could be vital in further analyzing the behavioral data.
 However many of these studies only account for network level characteristics
 of individuals, such as degree centrality,betweenness, prestige, clustering
 coefficient, etc
\begin_inset CommandInset citation
LatexCommand cite
key "hinz2011seeding"

\end_inset

.
 Although these measures are still valuable in predicting and studying the
 diffusion, and are more important than self-reported measures of opinion
 leadership
\begin_inset CommandInset citation
LatexCommand cite
key "iyengar2011opinion"

\end_inset

, in this paper we argue that sub-network level measures can provide further
 insights into recognizing more fine-grained centrality measures, and into
 recognizing topic related brokerage roles.
 Moreover, marketers are always interested in finding influential people
 in a network to be able to target them and segment them properly
\begin_inset CommandInset citation
LatexCommand cite
key "aral2012identifying,iyengar2011opinion,trusov2010determining,stephen2010deriving,van2001medical,van2007new,katona2011network"

\end_inset

; In addition to what previous literature has mostly involved with in recognizin
g the role of connectors, we believe the role of possible other individuals
 such as mavens and salesmen have been less attended in the context of social
 data
\begin_inset CommandInset citation
LatexCommand cite
key "gladwell2000tipping"

\end_inset

.
 Yet not all centrality measures on the network level correspond to similar
 notions and could provide even opposite results
\begin_inset CommandInset citation
LatexCommand cite
key "stephen2010deriving,trusov2010determining"

\end_inset

.
 Hence delving into more robust constructs at the community level might
 be a worthwhile investigation into the potential roles of specific individuals
 that otherwise would have not been identified.
 Detecting these sub-network properties can facilitate understanding the
 likelihood of information exchange and the attention given to information
\begin_inset CommandInset citation
LatexCommand cite
key "zubcsek2014information"

\end_inset

.
 Several studies have emerged in the field of marketing and management that
 pinpoint the importance of communities underlying social networks in better
 understanding consumer-firm or consumer-consumer relationships.
 Ansari et al (2011) model a multiplex network of professionals to simultaneousl
y study the impact of the organizational interventions on the nature of
 the connections
\begin_inset CommandInset citation
LatexCommand cite
key "ansari2011modeling"

\end_inset

.
 Ma et al (2014) use communities to account for homophily when studying
 the social influence of decision purchases and timing of individuals in
 a mobile network 
\begin_inset CommandInset citation
LatexCommand cite
key "ma2014latent"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Some skip latent homophily(first say the problem): Aral uses dynamic matched
 sample estimation by matching individuals on their observed characteristics,
 Snijders et al model co-evolution of network and control for selection
 using observed factors.
 , resolve with experiment Aral, ...
 why it is important(Shalizi).
\end_layout

\end_inset


\end_layout

\begin_layout Section
Model
\end_layout

\begin_layout Standard
In this paper we propose a model based approach to detect overlapping communitie
s as an extended version of mixed-membership-stochastic-blockmodel(MMSB)
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed,gopalan2013efficient"

\end_inset

, that allows for both scalable and efficient inference and more flexible
 community definitions.
 Mixed membership models provide tools to define a mixture over each grouped
 data
\begin_inset CommandInset citation
LatexCommand cite
key "erosheva2004mixed"

\end_inset

; a problem that mixture models tend to avoid by clustering data into separable
 groups that are conditionally independent of each other given their cluster
 assignment
\begin_inset CommandInset citation
LatexCommand cite
key "holland1983stochastic"

\end_inset

.
 First introduced in the context of topic discovery in text corpora, Blei
 et al (2003) defined distributions over the vocabulary, where underlying
 patterns define the topics, and each document is a distribution over these
 topics
\begin_inset CommandInset citation
LatexCommand cite
key "blei2003latent"

\end_inset

.
\end_layout

\begin_layout Standard
MMSB first proposed by Airoldi et al (2008)
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed"

\end_inset

, defines a generative setting for the formation of the links in a network.
 This model has been applied frequently to finding overlapping communities
 in social networks
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed,cho2016latent"

\end_inset

, protein-interaction networks
\begin_inset CommandInset citation
LatexCommand cite
key "airoldi2008mixed,gopalan2013efficient"

\end_inset

,citation networks
\begin_inset CommandInset citation
LatexCommand cite
key "cho2016latent"

\end_inset

, etc.
 The generative framework assumes that each individual in the network has
 different degrees of belonging to a set of 
\begin_inset Formula $K$
\end_inset

 pre-specified potentially overlapping communities.
 Among each directed pair of nodes(a potential link consisting of a sender
 and a receiver), sender 
\begin_inset Formula $s$
\end_inset

 activates one of its potential roles according to its membership strengths
 in different communities when communicating with receiver 
\begin_inset Formula $r$
\end_inset

.
 Likewise The receiver 
\begin_inset Formula $r$
\end_inset

 activates one of its roles according to its membership strengths in different
 communities when contacted by the sender 
\begin_inset Formula $s$
\end_inset

.
 This means that each individual can belong to several communities or take
 up different roles depending on whom they are contacting or being contacted
 by.
 According to the pair-based community announcements, a links is formed
 depending on the strength of the connection between those clusters.
 The data generating process is as follows:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Itemize
\begin_inset Formula $\forall a\in\mathcal{N}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
draw a 
\begin_inset Formula $K$
\end_inset

-dimensional mixed membership vector,
\begin_inset Formula $\boldsymbol{\theta}_{a}\sim Dirichlet(\boldsymbol{\alpha})$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\forall(a,b)\in\mathcal{E}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
draw one-hot membership indicator vector for 
\begin_inset Formula $a$
\end_inset

 when contacting 
\begin_inset Formula $b$
\end_inset

, 
\begin_inset Formula $\boldsymbol{z}_{a\rightarrow b}\sim Categorical(\boldsymbol{\theta}_{a})$
\end_inset


\end_layout

\begin_layout Itemize
draw one-hot membership indicator vector for 
\begin_inset Formula $b$
\end_inset

 when contacted by 
\begin_inset Formula $a$
\end_inset

, 
\begin_inset Formula $\boldsymbol{z}_{a\leftarrow b}\sim Categorical(\boldsymbol{\theta}_{b})$
\end_inset


\end_layout

\begin_layout Itemize
sample a link between 
\begin_inset Formula $a\rightarrow b$
\end_inset

 with probability 
\begin_inset Formula $\boldsymbol{z}_{a\rightarrow b}\boldsymbol{B}\boldsymbol{z}_{a\leftarrow b}$
\end_inset

, 
\begin_inset Formula $Y(a,b)\sim Bernoulli($
\end_inset


\begin_inset Formula $\boldsymbol{z}_{a\rightarrow b}\boldsymbol{B}\boldsymbol{z}_{a\leftarrow b}$
\end_inset

)
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MMSB data generating process
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:MMSB-data-generating"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the algorithm above the 
\begin_inset Formula $\boldsymbol{\theta}_{a}$
\end_inset

 represents a 
\begin_inset Formula $K$
\end_inset

-dimensional simplical vector of membership strength of node 
\begin_inset Formula $a$
\end_inset

 from a Dirichlet distribution, where 
\begin_inset Formula $a$
\end_inset

 is a member of the vertex set 
\begin_inset Formula $\mathcal{N}$
\end_inset

.
 For each directed pair 
\begin_inset Formula $(a,b)$
\end_inset

 that belong to the edge set 
\begin_inset Formula $\mathcal{E}$
\end_inset

, we acquire the indicator vector 
\begin_inset Formula $\boldsymbol{z},$
\end_inset

for each contact point from a categorical distribution that is parametrized
 by their membership strengths.
 Finally a diagonal Block(compatibility) matrix 
\begin_inset Formula $\boldsymbol{B}$
\end_inset

 determines the strength of inter-community connections based on what role
 is activated for each node.
 More generally in a repeated setting where links could frequent, we can
 replace the categorical distributions with the multinomial, and the block
 matrix 
\begin_inset Formula $\boldsymbol{B}$
\end_inset

 could entail any asymmetric and non-diagonal elements.
 Due to assortativity of many real world networks we assume here that 
\begin_inset Formula $\boldsymbol{B}$
\end_inset

 is diagonal.
 Several methods have been applied to estimate the model parameters, which
 among them variational inference
\begin_inset CommandInset citation
LatexCommand cite
key "jordan1999introduction,airoldi2008mixed,gopalan2013efficient"

\end_inset

 and MCMC
\begin_inset CommandInset citation
LatexCommand cite
key "chang2011lda,li2016scalable"

\end_inset

 are prevalent.
 Later in this section we discuss Variational method as it is the approach
 we take for our inference engine.
 Both variational methods and MCMC for this specific model have excelled
 to scale to very large networks through introducing stochastic mini-batch
 sampling
\begin_inset CommandInset citation
LatexCommand cite
key "hoffman2013stochastic,gopalan2013efficient,li2016scalable"

\end_inset

.Although widely applied all these models face some practical and technical
 limitations, that we aim to resolve only some of them in this paper.
 
\end_layout

\begin_layout Standard
One of the main limitations in most 
\shape italic
scalable
\shape default
 formulation of community detection under the assumptions of MMSB is that
 links are treated as undirected edges.
 We aim to recover communities by not disregarding the direction of the
 links;Although this might come with a cost, in the sense that we incorporate
 more information to process that makes the inference more computationally
 expensive, but we guarantee a scalable and efficient algorithm through
 unique mini-batch sampling within stochastic variational inference.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
More recently, but still limitations
\end_layout

\begin_layout Plain Layout
#VI,collapsed Gibbs
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Upon detecting communities as explained by the data generating process in
 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:MMSB-data-generating"

\end_inset

, individuals who belong to one community and not to a very similar one
 fail to connect to corresponding individuals due to small chance of connections
 between clusters.
 Although this is one of the more prevalent features of many community structure
s, it would be sensible if we allow for possibility of these connections.
 This may happen if some sort of hierarchy or nested structure exists in
 the nature of these communities that represent roughly their preferences,tastes
, roles, groups, etc.
 Although through variational inference the simpler model exhibits conjugacy
 and simplifies the estimation, a more natural way would be to allow for
 correlated mixed memberships by introducing a Logistic-Normal prior instead
 of Dirichlet for membership probabilities.
 This enables us to account for the connections among individuals that share
 rather similar interests or connections among communities that tend to
 interact more often compared to the specification in MMSB.
 In the context of mixed membership models , Lafferty and Blei (2006) introduced
 correlated topic models(CTM), that captures the correlation between topic
 proportions realized in a text corpora by incorporating a Logistic-Normal(LN)
 prior.
\begin_inset CommandInset citation
LatexCommand cite
key "lafferty2006correlated"

\end_inset

.
 In the case of our proposed MMSB variant, this would provide an advantage
 when moving from static to dynamic setting, where the LN-distributed parameters
 can change according to a simple autoregressive rule, that was not possible
 under the assumption of Dirichlet distribution
\begin_inset CommandInset citation
LatexCommand cite
key "blei2006dynamic,ho2011evolving,fu2009dynamic,xing2010state"

\end_inset

.
 Our proposed model is closely related to the LNMMSB in 
\begin_inset CommandInset citation
LatexCommand cite
key "xing2010state"

\end_inset

, however we take a hierarchical Bayesian perspective that allows for fully
 Bayesian variational inference, alongside preserving the direction of the
 links.
 The model is as follows:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Itemize
\begin_inset Formula $\forall k\in[1,..,K]$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
draw the diagonal elements of the block matrix 
\series bold

\begin_inset Formula $B$
\end_inset


\series default
 via
\begin_inset Formula $\beta_{k,k}\sim\mathcal{B}eta(\eta_{0},\eta_{1})$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\forall a\in\mathcal{N}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
draw the mean of the logit mixed membership vector through 
\begin_inset Formula $\boldsymbol{\mu}\sim\mathcal{N}ormal(\boldsymbol{\mu}_{0},\boldsymbol{\Lambda}_{0})$
\end_inset


\end_layout

\begin_layout Itemize
draw the precision of the logit mixed membership vector through 
\begin_inset Formula $\boldsymbol{\Lambda}\sim\mathcal{W}ishart(\boldsymbol{\ell}_{0},\boldsymbol{L}_{0})$
\end_inset


\end_layout

\begin_layout Itemize
draw a 
\begin_inset Formula $K$
\end_inset

-dimensional vector,
\begin_inset Formula $\boldsymbol{\theta}_{a}^{*}\sim\mathcal{N}ormal(\boldsymbol{\mu},\boldsymbol{\Lambda})$
\end_inset


\end_layout

\begin_layout Itemize
construct the simplical mixed membership via logistic transformation , 
\begin_inset Formula $\boldsymbol{\theta}_{a,k}=\frac{exp(\boldsymbol{\theta}_{a,k}^{*})}{\sum_{l}exp(\boldsymbol{\theta}_{a,l}^{*})}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\forall(a,b)\in\mathcal{E}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
draw one-hot membership indicator vector for 
\begin_inset Formula $a$
\end_inset

 when contacting 
\begin_inset Formula $b$
\end_inset

, 
\begin_inset Formula $\boldsymbol{z}_{a\rightarrow b}\sim Categorical(\boldsymbol{\theta}_{a})$
\end_inset


\end_layout

\begin_layout Itemize
draw one-hot membership indicator vector for 
\begin_inset Formula $b$
\end_inset

 when contacted by 
\begin_inset Formula $a$
\end_inset

, 
\begin_inset Formula $\boldsymbol{z}_{a\leftarrow b}\sim Categorical(\boldsymbol{\theta}_{b})$
\end_inset


\end_layout

\begin_layout Itemize
sample a link between 
\begin_inset Formula $a\rightarrow b$
\end_inset

 with probability 
\begin_inset Formula $\boldsymbol{z}_{a\rightarrow b}\boldsymbol{B}\boldsymbol{z}_{a\leftarrow b}$
\end_inset

, 
\begin_inset Formula $Y(a,b)\sim Bernoulli($
\end_inset


\begin_inset Formula $\boldsymbol{z}_{a\rightarrow b}\boldsymbol{B}\boldsymbol{z}_{a\leftarrow b}$
\end_inset

)
\end_layout

\end_deeper
\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Hierarchical-Logistic-Normal-MMSB
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:Hierarchical-Logistic-Normal-MMS"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The data generating process for potential link formation between two individuals
 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Hierarchical-Logistic-Normal-MMS"

\end_inset

 is shown in DAG in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DAG-structure-for"

\end_inset

.
 In this figure filled dots indicate model hyper-parameters, and shaded
 node 
\begin_inset Formula $y_{ab}$
\end_inset

 represents observed connection between two nodes.
 Directed arrows indicate the direction of the influence.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
usetikzlibrary{arrows}
\end_layout

\begin_layout Plain Layout


\backslash
tikzset{     
\end_layout

\begin_layout Plain Layout

	vertex/.style={circle,draw,minimum size=1.5em},
\end_layout

\begin_layout Plain Layout

    edge/.style={->,> = latex'} 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tikzpicture}
\end_layout

\begin_layout Plain Layout

% vertices 
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex,fill={rgb:black,1;white,2}] (y) at (2,2) {$y_{ab}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (b) at (2,0) {$
\backslash
beta_{kk}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex, shape=circle, fill, scale=.3] (eta) at (2,-2) {$
\backslash
eta$};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (2,-3) {$
\backslash
eta$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zab) at (0,4) {$z_{ab}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (zba) at (4,4) {$z_{ba}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (thetaa) at (0,6) {$
\backslash
theta^{*}_{a}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (thetab) at (4,6) {$
\backslash
theta^{*}_{b}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (mu) at (1,8) {$
\backslash
mu$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex] (Lambda) at (3,8) {$
\backslash
Lambda$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex, shape=circle, fill, scale=.3] (mu0) at (-.5,10) {$
\backslash
mu_0$};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (mu0.north) {$
\backslash
mu_0$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex, shape=circle, fill, scale=.3] (Lambda0) at (1.5,10) {$
\backslash
Lambda_0$};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (Lambda0.north) {$
\backslash
Lambda_0$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex, shape=circle, fill, scale=.3] (l0) at (2.5,10) {$
\backslash
ell_0$};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (l0.north) {$
\backslash
ell_0$};
\end_layout

\begin_layout Plain Layout


\backslash
node[vertex, shape=circle, fill, scale=.3] (L0) at (4.5,10) {$L_0$};
\end_layout

\begin_layout Plain Layout


\backslash
node [yshift=3.0ex, black] at (L0.north) {$L_0$};
\end_layout

\begin_layout Plain Layout

%
\backslash
node[draw=red, fit= (zi1) (zj1) (zi2) (zj2),  inner sep=.60cm, dashed, ultra
 thick, fill=red!10, fill %opacity=0.2] {};
\end_layout

\begin_layout Plain Layout

%
\backslash
node[draw=blue, fit= (zi1) (zj1) (zi2) (zj2) (zi3) (zj3),  inner sep=1.20cm,
 dashed, ultra thick, fill=blue!%10, fill opacity=0.2] {};
\end_layout

\begin_layout Plain Layout

%
\backslash
node [yshift=3.0ex, black] at (2.4,4.3) {Link Preferences};
\end_layout

\begin_layout Plain Layout

%
\backslash
node [yshift=3.0ex, black] at (-.5,4.8) {Latent Space};
\end_layout

\begin_layout Plain Layout

%
\backslash
node[draw=black, fit= (zi1) (zj1) (zi2) (zj2) (zi3) (zj3) (y) (Ai) (Aj),
  inner sep=1.40cm, dashed, ultra %thick, fill=black!10, fill opacity=0.2]
 {};
\end_layout

\begin_layout Plain Layout

%
\backslash
node [yshift=3.0ex, black] at (13,-3.5) {$t=1..T$};
\end_layout

\begin_layout Plain Layout

%edges 
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zi1) to [out=300,in=110] (y);
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zj1) to [out=240,in=70] (y);
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zi2) -- (y);
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zj2) -- (y);
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zi2) to [out=270,in=110] (Ai);
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zj2) to [out=270,in=70] (Aj);
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zi3) to [out=270,in=180] (Ai);
\end_layout

\begin_layout Plain Layout

%
\backslash
draw[edge] (zj3) to [out=270,in=0] (Aj);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (eta) to  (b);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (b) to  (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zab) to  (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (zba) to  (y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (thetaa) to  (zab);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (thetab) to  (zba);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (mu) to  (thetaa);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (Lambda) to  (thetaa);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (mu) to  (thetab);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (Lambda) to  (thetab);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (mu0) to  (mu);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (Lambda0) to  (mu);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (l0) to  (Lambda);
\end_layout

\begin_layout Plain Layout


\backslash
draw[edge] (L0) to  (Lambda);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
DAG structure for algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Hierarchical-Logistic-Normal-MMS"

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "fig:DAG-structure-for"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section*
Inference and Estimation
\end_layout

\begin_layout Standard
In this section we introduce the variational inference method.
 Other approaches fitting our Bayesian framework that could be used to handle
 the intractable posteriors,include Monte Carlo method
\begin_inset CommandInset citation
LatexCommand cite
key "neal1993probabilistic"

\end_inset

, and its variations including the Gibbs sampling, and Metropolis-Hastings.
 Although these algorithms provide theoretical guarantees on convergence,
 they may fail to converge in large parameter spaces as they need to process
 the full sample, and hence more recent methods including collapsed Gibbs
 sampler
\begin_inset CommandInset citation
LatexCommand cite
key "liu1994collapsed"

\end_inset

, or Hamiltonian Monte Carlo
\begin_inset CommandInset citation
LatexCommand cite
key "brooks2011handbook"

\end_inset

 were developed to address this issue.
 For more examples of scalable Monte Carlo methods see
\begin_inset CommandInset citation
LatexCommand cite
key "girolami2011riemann,patterson2013stochastic,welling2011bayesian"

\end_inset

, and for an example of the use of Stochastic Riemannian Langevin Dynamics
 Monte Carlo applied to the problem of community detection see
\begin_inset CommandInset citation
LatexCommand cite
key "li2016scalable"

\end_inset

.
\end_layout

\begin_layout Standard
Instead we use the variational inference, widely applied in the realm of
 probabilistic inference and parameter learning, which transforms the problem
 of inference to an optimization one, by trying to minimize the Kullback-Leibler
 divergence between the true posterior distribution 
\begin_inset Formula $p$
\end_inset

 and a simplified proposed variational distribution 
\begin_inset Formula $q$
\end_inset

.
 Hence instead of making exact inference through approximation, variational
 inference tend to offer deterministic approximation to the the model posterior
 distribution.
 In its simplest case, the proposed model follows a mean field assumption,
 where it tries to decouple parameters in a way that we can still have tractable
 results, close enough to the true posterior.
 In a fully Bayesian framework, where we set all the latent variable to
 be 
\begin_inset Formula $\boldsymbol{Z}$
\end_inset

 and all the observed variables to be 
\begin_inset Formula $\boldsymbol{X}$
\end_inset

, we specify a joint probability model 
\begin_inset Formula $P(\boldsymbol{X},\boldsymbol{Z})$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
We have dropped the model parameters 
\begin_inset Formula $\theta$
\end_inset

, only to avoid cluttered notation.
\end_layout

\end_inset

, and our goal is to find an approximation to the true posterior 
\begin_inset Formula $P(\boldsymbol{Z}|\boldsymbol{X})$
\end_inset

 and also our model evidence 
\begin_inset Formula $P(\boldsymbol{X}).$
\end_inset

The log likelihood of the model follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P(\boldsymbol{X}) & =\int_{\boldsymbol{Z}}P(\boldsymbol{X},\boldsymbol{Z})d\boldsymbol{Z}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We further use the log transformation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
ln\,P(\boldsymbol{X}) & =ln\,\int_{\boldsymbol{Z}}P(\boldsymbol{X},\boldsymbol{Z})d\boldsymbol{Z}\nonumber \\
 & \overbrace{=}^{\times\frac{q(\boldsymbol{Z})}{q(\boldsymbol{Z})}}ln\,\int_{\boldsymbol{Z}}P(\boldsymbol{X},\boldsymbol{Z})\times\frac{q(\boldsymbol{Z})}{q(\boldsymbol{Z})}d\boldsymbol{Z}\nonumber \\
 & =ln\,\mathbb{E}_{q}\Bigg[\frac{P(\boldsymbol{X},\boldsymbol{Z})}{q(\boldsymbol{Z})}\Bigg]\label{eq:logpx}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Since the logarithm is a concave function, Jensen equality could be applied
 to get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
ln\,P(\boldsymbol{X}) & =ln\,\mathbb{E}_{q}\Bigg[\frac{P(\boldsymbol{X},\boldsymbol{Z})}{q(\boldsymbol{Z})}\Bigg]\nonumber \\
 & \geq\mathbb{E}_{q}\Big[ln\,P(\boldsymbol{X},\boldsymbol{Z})\Big]-\mathbb{E}_{q}\Big[ln\,q(\boldsymbol{Z})\Big]=\mathcal{L}(q)\label{eq:elbo}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:elbo"

\end_inset

 is known as the evidence lower bound(ELBO).
 Note that we can write the equation 1
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:logpx"

\end_inset

 in the following format following the fact that 
\begin_inset Formula $P(\boldsymbol{X},\boldsymbol{Z})=P(\boldsymbol{Z}|\boldsymbol{X})P(\boldsymbol{X})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
ln\,P(\boldsymbol{X}) & =ln\,\mathbb{E}_{q}\Bigg[\frac{P(\boldsymbol{X},\boldsymbol{Z})}{q(\boldsymbol{Z})}\Bigg]\nonumber \\
 & =\mathbb{E}_{q}\Big[ln\,P(\boldsymbol{X},\boldsymbol{Z})\Big]-\mathbb{E}_{q}\Big[ln\,q(\boldsymbol{Z})\Big]+\mathbb{E}_{q}\Big[ln\,q(\boldsymbol{Z})\Big]-\mathbb{E}_{q}\Big[ln\,P(\boldsymbol{Z}|\boldsymbol{X})\Big]\label{eq:margdecom}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\mathbb{E}_{q}\Big[ln\,q(\boldsymbol{Z})\Big]-\mathbb{E}_{q}\Big[ln\,P(\boldsymbol{Z}|\boldsymbol{X})\Big]$
\end_inset

 in equation
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:margdecom"

\end_inset

 is equivalent to the Kullback-Leibler divergence of the proposed variational
 distribution 
\begin_inset Formula $q(\boldsymbol{Z})$
\end_inset

 and the true posterior 
\begin_inset Formula $P(\boldsymbol{Z}|\boldsymbol{X})$
\end_inset

.
 Hence we can rewrite the log marginal as the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
ln\,P(\boldsymbol{X}) & =\mathcal{L}(q)+KL\Big(q(\boldsymbol{Z})||P(\boldsymbol{Z}|\boldsymbol{X})\Big)\label{eq:margdecomshort}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
To simplify the Kullback-Leibler divergence we can rewrite 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
KL\Big(q(\boldsymbol{Z})||P(\boldsymbol{Z}|\boldsymbol{X})\Big) & =-\Bigg(\mathbb{E}_{q}\Big[ln\,P(\boldsymbol{X},\boldsymbol{Z})\Big]-\mathbb{E}_{q}\Big[ln\,q(\boldsymbol{Z})\Big]\Bigg)+ln\,P(\boldsymbol{X})\nonumber \\
 & =-\mathcal{L}(q)+ln\,P(\boldsymbol{X})\label{eq:kldecom}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
According to equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kldecom"

\end_inset

, minimization the KL-divergence between the variational distribution and
 the true posterior translates to maximization the ELBO as the marginal
 distribution 
\begin_inset Formula $P(\boldsymbol{X}),$
\end_inset

does not depend on the variational distribution 
\begin_inset Formula $q$
\end_inset

.
 We generally in practice tackle the simpler problem of maximizing the lower
 bound instead of minimizing the KL divergence;however as can be seen in
 equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kldecom"

\end_inset

 this exactly corresponds to our goal of minimizing the distance between
 the true intractable posterior and the simpler variational distribution
\begin_inset Foot
status open

\begin_layout Plain Layout
For other bounds see ?
\end_layout

\end_inset

.
 Hence the problem of finding the posterior has been reduced to one that
 tightens the lower bound.
 The closeness depends on our approximating distribution and how well it
 can represent the true posterior and yet is tractable.
 There is much discussion about the performance of the approximation through
 variational inference and exact methods(?,?,?).
 However in practice variational methods have shown to perform at least
 as good as other methods, and yet they can easily be adjusted to scale
 to very large datasets
\begin_inset CommandInset citation
LatexCommand cite
key "salimans2015markov"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "blei2017variational"

\end_inset

(??).
\end_layout

\begin_layout Standard
On asymptotic properties of the variational inference???
\end_layout

\begin_layout Standard
In the following section we derive the variational updates for our current
 model of Logistic-Normal MMSB described above, and later we adjust the
 algorithm to scale to larger data sets by using stochastic search.
\end_layout

\begin_layout Section*
Variational Algorithm for Directed LNMMSB
\end_layout

\begin_layout Standard
In this section we develop the variational algorithm for the case of directed
 network Logistic-Normal MMSB.
 For this we start by writing down the ELBO which entails the variational
 expectation of the joint and approximating distributions.
 The log joint distribution of data, latent variables and model parameters
 as suggested by Algorithm
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Hierarchical-Logistic-Normal-MMS"

\end_inset

 follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
ln\,p(joint) & =ln\,p(\mu|m_{0},M_{0})+ln\,p(\Lambda|\ell_{0},L_{0})+\sum_{a}ln\,p(\theta_{a}|\mu,\Lambda)+\sum_{a}\sum_{b}ln\,p(z_{a\rightarrow b}|\theta_{a})\nonumber \\
 & +\sum_{a}\sum_{b}ln\,p(z_{a\leftarrow b}|\theta_{b})+\sum_{k}ln\,p(\beta_{kk}|\eta_{0},\eta_{1})+\sum_{a}\sum_{b}ln\,p(y_{ab}|z_{a\rightarrow b},z_{a\leftarrow b},\beta)\label{eq:logjoint}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Furthermore we assume that our approximating distributions follows a mean
 field assumption
\begin_inset CommandInset citation
LatexCommand cite
key "wainwright2008graphical,jordan1999introduction"

\end_inset

, where the probability distribution is factorized.
 The variational parameters are distributed as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mu & \sim & q(\mu|m,M)\sim\mathcal{N}ormal(\mu|m,M)\nonumber \\
\Lambda & \sim & q(\Lambda|\ell,L)\sim\mathcal{W}ishart(\Lambda|\ell,L)\nonumber \\
\theta_{a} & \sim & q(\theta_{a}|\mu_{a},\Lambda_{a})\sim\mathcal{N}ormal(\theta_{a}|\mu_{a},\Lambda_{a})\nonumber \\
\beta_{kk} & \sim & q(\beta_{kk}|b_{k})\sim\mathcal{B}eta(b_{k0},b_{k1})\nonumber \\
z_{a\rightarrow b} & \sim & q(z_{a\rightarrow b}|\phi_{a\rightarrow b})\sim Categorical(z_{a\rightarrow b}|\phi_{a\rightarrow b})\nonumber \\
z_{a\leftarrow b} & \sim & q(z_{a\leftarrow b}|\phi_{a\leftarrow b})\sim Categorical(z_{a\leftarrow b}|\phi_{a\leftarrow b})\label{eq:indivvardists}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Following shows the logarithmic transformation of the factorized variational
 distribution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
ln\,q(.) & =ln\,q(\mu|m,M)+ln\,q(\Lambda|\ell,L)+\sum_{a}ln\,q(\theta_{a}|\mu_{a},\Lambda_{a})+\sum_{a}\sum_{b}ln\,q(z_{a\rightarrow b}|\phi_{a\rightarrow b})\nonumber \\
 & +\sum_{a}\sum_{b}ln\,q(z_{a\leftarrow b}|\phi_{a\leftarrow b})+\sum_{k}ln\,q(\beta_{kk}|b_{k0},b_{k1})\label{eq:logvardist}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Taking the variational expectation for the log joint probability distribution
 of the model leads to the negative cross entropy for the corresponding
 distribution and the variational expectation of the factorized distribution
 leads to the corresponding negative entropies.
 To see how cross entropies and entropies are derived refer to Appendix(?).
 After taking the relevant expectation for each probability distribution
 the ELBO becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mathcal{L} & = & \mathbb{E}_{q}[ln\,p(\mu|m_{0},M_{0})]+\mathbb{E}_{q}[ln\,p(\Lambda|\ell_{0},L_{0})]+\sum_{a}\mathbb{E}_{q}[ln\,p(\theta_{a}|\mu,\Lambda)]+\nonumber \\
 &  & \sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,p(z_{a\rightarrow b}|\theta_{a})]+\sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,p(z_{a\leftarrow b}|\theta_{b})]+\nonumber \\
 &  & \sum_{k}\mathbb{E}_{q}[ln\,p(\beta_{kk}|\eta)]+\sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,p(y_{ab}|z_{a\rightarrow b},z_{a\leftarrow b},\beta)]\nonumber \\
 &  & +H_{q}[\mu]+H_{q}[\Lambda]+H_{q}[\theta]+H_{q}[\beta]+H_{q}[z_{\rightarrow}]+H_{q}[z_{\leftarrow}]\label{eq:fullelbo}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In the extension of above full ELBO in Appendix(?), we differentiate between
 the 
\begin_inset Formula $\phi$
\end_inset

's for links versus for non-links.
 This can come handy when we use mini-batch sampling and extend the algorithm
 to be stochastic.
\end_layout

\begin_layout Standard
Explanation of the parameter names...
\begin_inset Formula $N$
\end_inset

 corresponds to the number of nodes and 
\begin_inset Formula $K$
\end_inset

 to the number of communities.
\end_layout

\begin_layout Standard
Note that in line 2 of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fullelbo"

\end_inset

, we can use the Jensen equality to bound the expectation of log sum of
 exponentials 
\begin_inset Formula $\mathbb{E}_{q}\Bigg[ln\,\sum_{l}exp(\theta_{a,l})\Bigg]$
\end_inset

, since the concavity of the logarithm allows us to write 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\mathbb{E}_{q}\Bigg[ln\,\sum_{l}exp(\theta_{a,l})\Bigg] & \leq ln\,\Bigg(\sum_{l}\mathbb{E}_{q}\big[exp(\theta_{a,l})\big]\Bigg)\nonumber \\
 & =ln\,\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\Lambda_{a,l}^{-1})\label{eq:secondjensen}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For further details about the variational expectation of this terms refer
 to Appendix(??).This is one of the caveats of using the Logistic-Normal
 prior instead of the Dirichlet distribution, introducing non-conjugacy
 that hinders us from deriving fully closed-form solution for the variational
 parameters involved.
\end_layout

\begin_layout Standard
To derive each variational parameter, we maximize the ELBO with respect
 to that parameter.
 For the full derivation of each parameter refer to the Appendix(?).
 Variational equations boil down to updates for global and local parameters.
 Global variational parameters are model specific(
\begin_inset Formula $m,M,\ell,L,b$
\end_inset

), whereas local parameters are individual and observation specific(
\begin_inset Formula $\phi_{a\rightarrow b},\phi_{a\leftarrow b},\mu_{a},\Lambda_{a}$
\end_inset

).
\end_layout

\begin_layout Subsection*

\series bold
Global Parameters
\end_layout

\begin_layout Standard
To derive the variational mean and precision of the mean membership strength
 we optimize the ELBO with respect to 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $M$
\end_inset

 to get 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
m & =(M_{0}+N\ell L)^{-1}(M_{0}m_{0}+\ell L\sum_{a}\mu_{a})\nonumber \\
M & =M_{0}+N\ell L\label{eq:m&M}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Additionally optimizing the degrees of freedom and scale matrix of the Wishart
 distributed precision simultaneously, we have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
L= & \Bigg((L_{0}^{-1}+\sum_{a}\Lambda_{a}^{-1}+\sum_{a}(\mu_{a}-m)(\mu_{a}-m)^{T}+NM^{-1})\Bigg)^{-1}\nonumber \\
\ell= & \ell_{0}+N\label{eq:l&L}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Simultaneously optimizing the ELBO with respect to 
\begin_inset Formula $b_{k0}$
\end_inset

 and 
\begin_inset Formula $b_{k1}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
b_{k0} & =\eta_{0}+\sum_{a,b\in links}\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\nonumber \\
b_{k1} & =\eta_{1}+\sum_{a,b\notin links}\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\label{eq:b0b1}
\end{align}

\end_inset


\end_layout

\begin_layout Subsection*

\series bold
Local Parameters
\end_layout

\begin_layout Standard
\begin_inset Formula $\phi$
\end_inset

 parameters correspond to the membership strength in the variational specificati
on, and as in the generative process within each community they have to
 add to unity(i,e
\begin_inset Formula $\sum_{l}\phi_{a\rightarrow b,l}$
\end_inset

).
 We can only maximize the ELBO with respect to these parameters up to a
 scale.
 Hence after finding the non-normalized 
\begin_inset Formula $\phi$
\end_inset

's we make a normalizing transformation to make sure that they sum to one.
\end_layout

\begin_layout Standard
For the link specific updates:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\phi_{a\rightarrow b,k} & \propto exp\Bigg\{\mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)\Bigg\}\label{eq:linksend}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\phi_{a\leftarrow b,k} & \propto exp\Bigg\{\mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)\Bigg\}\label{eq:linkrecv}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Similarly for the non-links:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\phi_{a\rightarrow b,k} & \propto exp\Bigg\{\mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)\Bigg\}\label{eq:nlinksend}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\phi_{a\leftarrow b,k} & \propto exp\Bigg\{\mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)\Bigg\}\label{eq:nlinkrecv}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
There is no closed form solution for either 
\begin_inset Formula $\mu_{a}$
\end_inset

 or 
\begin_inset Formula $\Lambda_{a}$
\end_inset

 due to the choice of the non-conjugate prior.
 Hence we derive the gradients of the ELBO with respect to both parameters
 and use Adagrad
\begin_inset CommandInset citation
LatexCommand cite
key "duchi2011adaptive"

\end_inset

.
 
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\nabla_{\mu_{a}}\mathcal{L}_{\mu_{a}}=$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align}
-\ell L(\mu_{a}-m)+\sum_{b\in sink(a)}\phi_{a\rightarrow b}+\sum_{b\notin sink(a)}\phi_{a\rightarrow b}\nonumber \\
+\sum_{b\in source(a)}\phi_{b\leftarrow a}+\sum_{b\notin source(a)}\phi_{b\leftarrow a}-\sum_{b}\underbar{sfx}(a)\label{eq:mugrad}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\underbar{sfx}(a)$
\end_inset

 refers to a vector function of dimension 
\begin_inset Formula $K$
\end_inset

, and each element represents the soft-max transformation of 
\begin_inset Formula $(\mu_{a}+\tfrac{1}{2}\Lambda_{a}^{-1})$
\end_inset


\end_layout

\begin_layout Standard
Instead of optimizing with respect to the precision we maximize the ELBO
 with respect to the diagonal covariance as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula $\nabla_{\Lambda_{a}^{-1}}\mathcal{L}_{\Lambda_{a}^{-1}}=$
\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{align}
-\tfrac{\ell}{2}diag(L)+\tfrac{1}{2}(\Lambda_{a})-\tfrac{1}{2}\sum_{b}(\underbar{sfx}(a))\label{eq:Lambdagrad}\\
\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For further details on the gradients of the ELBO with respect to 
\begin_inset Formula $\mu_{a}$
\end_inset

 and 
\begin_inset Formula $\Lambda_{a}$
\end_inset

 see Appendix(?).
\end_layout

\begin_layout Standard
The variational procedure is summarized in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Variational-Algorithm"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Itemize
Until ELBO improves
\end_layout

\begin_deeper
\begin_layout Itemize
Initialize the variational parameters
\end_layout

\begin_layout Itemize
update local parameters
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\forall(a,b)\in\mathcal{E}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
update 
\begin_inset Formula $\phi_{a\rightarrow b,k}$
\end_inset

 and 
\begin_inset Formula $\phi_{a\leftarrow b,k}$
\end_inset

 according to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:linksend"

\end_inset

,and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:linkrecv"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\forall(a,b)\notin\mathcal{E}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
update 
\begin_inset Formula $\phi_{a\rightarrow b,k}$
\end_inset

 and 
\begin_inset Formula $\phi_{a\leftarrow b,k}$
\end_inset

 according to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:nlinksend"

\end_inset

,and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:nlinkrecv"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\forall a\in\mathcal{N}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
update 
\begin_inset Formula $\mu_{a}$
\end_inset

 and 
\begin_inset Formula $\Lambda_{a}$
\end_inset

 using the gradients 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mugrad"

\end_inset

, and
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Lambdagrad"

\end_inset

 via Adagrad
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
update global parameters
\end_layout

\begin_deeper
\begin_layout Itemize
update the global parameters 
\begin_inset Formula $m,M,\ell,L,b_{0},b_{1}$
\end_inset

 according to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:m&M"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:l&L"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:b0b1"

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
end
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Variational Algorithm
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:Variational-Algorithm"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section*
Stochastic Variational Algorithm
\end_layout

\begin_layout Standard
Variational inference offers a fast approximation of the posterior distribution
 by optimizing the ELBO, however this might need the screening of the whole
 individual(link) level observations both for updating the variational parameter
s and also evaluating the ELBO.
 A more recent method in variational inference offers a stochastic search
 in the parameter space suggested by 
\begin_inset CommandInset citation
LatexCommand cite
key "hoffman2013stochastic"

\end_inset

.
 Stochastic Variational Inference(SVI) instead of the full sample, samples
 only a small mini-batch from the data and reweighs the parameter updates
 according to a decay rate that satisfies the conditions of stochastic search
 by Robbins and Monro
\begin_inset CommandInset citation
LatexCommand cite
key "robbins1951stochastic"

\end_inset

.
 According to SVI, our algorithm only needs to iterate between sub-sampling
 the data to acquire a noisy gradient of the objective function and only
 update according to the sub-sampled mini-batch.
 Under the updating steps conditions the algorithm can provably converge
 to the optimum
\begin_inset CommandInset citation
LatexCommand cite
key "hoffman2013stochastic"

\end_inset

.
 For further examples of this approach in topic modeling refer to
\begin_inset CommandInset citation
LatexCommand cite
key "cho2016latent,gopalan2013efficient,hoffman2013stochastic"

\end_inset

.
 Related to our work, Gopalan et al (2013) offers several sub-sampling schemes,
 including the link-only sampling which provides both efficient and reasonably
 appropriate simplification for undirected networks
\begin_inset CommandInset citation
LatexCommand cite
key "gopalan2013efficient"

\end_inset

.
 However for the case of directed network the assumption of link-only sampling
 although may offer more convenience, can be too simplistic and result in
 biased estimates.
 In the following section we introduce our sampling scheme.
\end_layout

\begin_layout Subsection*
Mini-batch Sampling
\end_layout

\begin_layout Standard
In our model, we sample our network using only nodes, and since large networks
 exhibit a very sparse patterns of connections, each time we sample few
 nodes with all their links and equal proportion(or a small multiple) of
 their randomly selected non-links.
 After rounds of iteration this assumption both takes into account the informati
on of all links, and actual non-links.
 Moreover the random selection of the non-links allows different non-links
 to be visited.
 We argue that averaging method of 
\begin_inset CommandInset citation
LatexCommand cite
key "gopalan2013efficient,li2016scalable"

\end_inset

 for the non-link 
\begin_inset Formula $\phi$
\end_inset

's can introduce biases regarding our estimates, as with directed networks,
 the amount of information in the direction of links and non-links cannot
 easily be ignored.
 
\end_layout

\begin_layout Standard
For this purpose we use informative stratified random node sampling, where
 we define two types of sets for each node:informative, and non-informative
 sets.
 Informative set consists of all the links that involve a specific node,
 and non-links of that node that at most 
\begin_inset Formula $h$
\end_inset

 hops away.Non-informative set on the other hand consists of 
\begin_inset Formula $S$
\end_inset

 sets that that partitions the rest of the non-links that involve that node.
 For the sake of mini-batch sampling, we first sample a node at random,
 and with a high chance select the informative set of that node, and with
 a very low probability we choose one of its 
\begin_inset Formula $S$
\end_inset

 non-informative non-link sets.
 Regarding each update we use this information to reweigh the computation
 of each expectation in the updates.
 For example to compute the variational expectation of the link probabilities,
 we have to reweigh that with 
\begin_inset Formula $p_{high}\times\frac{1}{2N}$
\end_inset

 if it comes from a link set or 
\begin_inset Formula $(1-p_{high})\times\frac{1}{2NS}$
\end_inset

 if it comes from the non-link set.
\end_layout

\begin_layout Standard
At each iteration we sample a small number of nodes at random, include all
 their links and a equal number of randomly selected non-links of those
 nodes, and accordingly we reweigh our previous parameter updates based
 on the mini-batch sizes.
\end_layout

\begin_layout Section*
Parameter Initialization
\end_layout

\begin_layout Section*
Data
\end_layout

\begin_layout Standard
We start our algorithm by using the variational inference on a small synthetic
 network, and then we apply the stochastic version of the algorithm to this
 data.
 Due to the inability of traditional variational inference to comply with
 large networks, we apply our stochastic variational inference to a very
 large synthetic network.
 To extend our findings in real network datasets, we use a relatively large
 network of political blog citations, and a very large bookmarking blog.
\end_layout

\begin_layout Subsection*

\series bold
Training set and holdout set
\end_layout

\begin_layout Standard
At each configuration of datasets, we divide our data to training and validation
 sets, where in the training set, we employ the learning and see the performance
 in the validation set, and also use the validation to learn the model parameter
s.
 Except in the traditional variational inference with the synthetic networks,
 where we use normalized mutual information(NMI) to assess the performance
 between our estimates and the ground truth values.
 Our holdout set consists of 2.5% of links and the equal number of non-links,
 and the rest would be considered our training data.
\end_layout

\begin_layout Subsection*
Synthetic Data
\end_layout

\begin_layout Subsubsection*
small network
\end_layout

\begin_layout Standard
We construct a small synthetic network, consisting of 250 individuals in
 8 relatively dense communities portrayed in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:small-synthetic-network"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/arashyazdiha/Dropbox/Arash/EUR/Workspace/CLSM/Julia Implementation/ProjectLN/Docs/net250.png
	scale 30
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
small synthetic network visualization 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:small-synthetic-network"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
large network
\end_layout

\begin_layout Subsection*
Real World Data
\end_layout

\begin_layout Subsubsection*
political blog citation
\end_layout

\begin_layout Subsubsection*
bookmarking network
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Subsection*
synthetic
\end_layout

\begin_layout Subsubsection*
Small network
\end_layout

\begin_layout Subsubsection*
Performance Checking
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ELBO-of-small"

\end_inset

 show the improvement of the evidence lower bound as we maximize it, which
 translates to minimizing the KL-divergence of the two distribution.
 As we can see the model stays steady and flat after 100 rounds of iterations,
 which means that the engine has already converged.
 In the case where we can evaluate the ELBO in a small network, checking
 the behavior of the lower bound is the first step towards the performance
 check of the model.
 This might still be a steady state where convergence has arrived at a non-escap
able settings in the parameter space.
 The monotonic increase in the ELBO signifies the correct direction of the
 steps taken in the inference.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/arashyazdiha/Dropbox/Arash/EUR/Workspace/CLSM/Julia Implementation/ProjectLN/Docs/ELBO0png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ELBO of small synthetic network
\begin_inset CommandInset label
LatexCommand label
name "fig:ELBO-of-small"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We estimate the membership probabilities using the variational distribution
 and compare it with the ground truth values.
 Although the initialization algorithm starts by detecting 11 communities,
 we further investigate those clusters after estimation, which yields 3
 empty clusters.
 For better visual comparison, we excluded those empty communities and plotted
 the membership probabilities of each individual in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:thetaestimates-from"

\end_inset

.
 The plot visually provides a reasonable closeness of our estimates and
 the ground truth for a small network.
 However for further assessments we need to quantify this performance.
 Next section introduces the Normalized Mutual Information, that is commonly
 used in evaluating the performance of overlapping community detection algorithm
s provided the ground truth.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
or 
\begin_inset Graphics
	filename /home/arashyazdiha/Dropbox/Arash/EUR/Workspace/CLSM/Julia Implementation/ProjectLN/Docs/thetaest0.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $\theta$
\end_inset

 estimates from VI(top), and the true 
\begin_inset Formula $\theta$
\end_inset

's(bottom)
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:thetaestimates-from"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Normalized Mutual Information(NMI)
\end_layout

\begin_layout Standard
For the data with ground truth we can also use normalized mutual information(NMI
).
 NMI has been used vastly in assessing the performance of algorithms involving
 overlapping clustering
\begin_inset CommandInset citation
LatexCommand cite
key "mcdaid2011normalized,lancichinetti2009detecting"

\end_inset

.The table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Normalized-Mutual-Information"

\end_inset

 shows the NMI values for different set of comparisons.
 As we can see the in the last row, our estimated clusters shows prominent
 improvement from the well initialized algorithm.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
case
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NMI
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape italic
truth 
\shape default
vs.
 
\shape italic
initialized
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.438348
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
estimated vs.
 
\shape italic
initialized
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.361077
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
estimated vs.
 
\shape italic
truth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.804357
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Normalized Mutual Information for small synthetic network
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "tab:Normalized-Mutual-Information"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Large network.
\end_layout

\begin_layout Subsubsection*
Performance check
\end_layout

\begin_layout Standard
Perplexity of a model 
\begin_inset Formula $q$
\end_inset

, given a stochastic process 
\begin_inset Formula $p$
\end_inset

 (here a joint generative model that can produce infinite streams of data)
 is defined as: 
\end_layout

\begin_layout Standard
\begin_inset Formula $perplexity(p,q)\triangleq2^{H(p,q)}$
\end_inset


\end_layout

\begin_layout Standard
where the cross entropy 
\begin_inset Formula 
\begin{align*}
H(p,q) & \triangleq lim_{N\rightarrow\infty}-\tfrac{1}{N}\sum_{y_{1:N}}p(y_{1:N})log\,q(y_{1:N})\\
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and perplexity the lower the better.
 But we use the empirical distribution for p.
 
\begin_inset Formula $p_{emp}(y_{1:N})=\delta_{y_{1:N}^{*}}(y_{1:N}),$
\end_inset

where 
\begin_inset Formula $y^{*}$
\end_inset

 is a single long test sequence.
 Then 
\begin_inset Formula $H(p_{emp},q)=-\tfrac{1}{N}log\,q(y_{1:N}^{*})$
\end_inset

, so the perplexity becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
perplexity(p_{emp},q) & = & q(y_{1:N}^{*})^{1/N}=^{N}\sqrt{\prod_{i=1}^{N}\dfrac{1}{q(y_{i}^{*}|y_{1:i-1}^{*})}}\\
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
which is the geometric mean of the inverse of the predictive probability.
\end_layout

\begin_layout Standard
We could also think of it as exponential of the negative average log likelihood
 of the data.
 average perplexity on a test set is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
perp_{avg}(Test|params) & = & exp\,\Bigg(-\dfrac{\sum_{(a,b)\in Test}log\,(\tfrac{1}{T})\sum_{t=1}^{T}p(y_{ab}|params)}{|Test|}\Bigg)\\
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can set for example a 1% of the links and the non-links as the test set.
\end_layout

\begin_layout Standard
Real data
\end_layout

\begin_layout Standard
political blog citation 
\end_layout

\begin_layout Standard
bookmarkig network
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "biblio"
options "plain"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
\start_of_appendix
\noindent
Appendix
\end_layout

\begin_layout Section
\noindent
Negative cross entropies
\end_layout

\begin_layout Subsection
\noindent
Two Normals
\end_layout

\begin_layout Standard
\noindent
Note:All the normals are parametrized using the precision matrix.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $q\sim\mathcal{N}(x|m,L)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $p\sim\mathcal{N}(x|\mu,\Lambda)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\int q(x)ln\,p(x)dx & = & \int\mathcal{N}(x|m,L)\times\\
 &  & \Bigg(-\tfrac{K}{2}ln\,2\pi+\tfrac{1}{2}ln\,|\Lambda|-\\
 &  & \tfrac{1}{2}\Big(Tr\,\Lambda\{(x-\mu)(x-\mu)^{T}\}\Big)\Bigg)dx\\
 & = & -\tfrac{K}{2}ln\,2\pi+\tfrac{1}{2}ln\,|\Lambda|+\\
 &  & \int\mathcal{N}(x|m,L)\Bigg(-\tfrac{1}{2}\Big(Tr\,\Lambda\{(x-\mu)(x-\mu)^{T}\}\Big)\Bigg)dx-\\
 & = & \tfrac{K}{2}ln\,2\pi+\tfrac{1}{2}ln\,|\Lambda|+\\
 &  & \int\mathcal{N}(x|m,L)\Bigg(-\tfrac{1}{2}\Big(Tr\Lambda\{xx^{T}+\mu\mu^{T}-x\mu^{T}-\mu x^{T}\}\Big)\Bigg)dx
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
We should note that 
\begin_inset Formula $\mathbb{E}_{q}\Big[xx^{T}\Big]=Cov_{q}+\mathbb{E}_{q}\Big[x\Big]\mathbb{E}_{q}\Big[x\Big]^{T}$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mathbb{E}_{q}\Big[x\Big]=m$
\end_inset

 and 
\begin_inset Formula $Cov_{q}=L^{-1}$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\int\mathcal{N}(x|m,L)\Bigg(-\tfrac{1}{2}\Big(Tr\,\Big[\Lambda\{xx^{T}+\mu\mu^{T}-x\mu^{T}-\mu x^{T}\}\Big]\Big)\Bigg)dx=\\
-\tfrac{1}{2}Tr\,\Big[(\Lambda L^{-1}+\Lambda mm^{T})+\Lambda(mm^{T}-\mu m^{T}-m\mu^{T})\Big]\\
=-\tfrac{1}{2}\Big(Tr\,\Big[\Lambda L^{-1}\Big]+(m-\mu)^{T}\Lambda(m-\mu)\Big)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Hence we have:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\boxed{{\mathbb{E}_{q}[ln\,p(x)]=-\tfrac{K}{2}ln\,2\pi+\tfrac{1}{2}ln\,|\Lambda|-\tfrac{1}{2}\Big(Tr\,\Big[\Lambda L^{-1}\Big]+(m-\mu)^{T}\Lambda(m-\mu)\Big)}}$
\end_inset


\end_layout

\begin_layout Subsection
\noindent
Two Wisharts
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\Lambda\sim q\sim\mathcal{W}(v,W)$
\end_inset


\end_layout

\begin_layout Standard
\noindent

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $\Lambda\sim p\sim\mathcal{W}(n,S)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\int q(\Lambda)ln\,p(\Lambda)d\Lambda & = & \mathbb{E}_{q}[ln\,p(\Lambda)]\\
 & = & \mathbb{E}_{q}\Bigg[ln\,\frac{|\Lambda|^{\tfrac{n-K-1}{2}}exp(-\tfrac{1}{2}Tr\,(S^{-1}\Lambda)}{2^{\tfrac{nK}{2}}|S|^{n/2}\Gamma_{p}(\tfrac{n}{2})}\Bigg]\\
 & = & \mathbb{E}_{q}\Bigg[-\tfrac{nk}{2}ln\,2-\tfrac{n}{2}ln\,|S|-ln\,\Gamma_{K}(\tfrac{n}{2})\\
 &  & +\tfrac{n-K-1}{2}ln\,|\Lambda|-\tfrac{1}{2}Tr\,(S^{-1}\Lambda)\Bigg]\\
 & = & -\tfrac{nk}{2}ln\,2-\tfrac{n}{2}ln\,|S|-ln\,\Gamma_{K}(\tfrac{n}{2})\\
 &  & +\tfrac{n-K-1}{2}\Big(\psi_{K}(\tfrac{v}{2})+Kln\,2+ln\,|W|\Big)-\tfrac{v}{2}Tr\,(S^{-1}W)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mathbb{E}_{q}[\Lambda]=vW$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mathbb{E}_{q}[ln\,|\Lambda|]=\psi_{K}(\tfrac{v}{2})+Kln\,2+ln\,|W|$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\psi_{K}(\tfrac{v}{2})=\sum_{i:1}^{K}\psi(\tfrac{v-i+1}{2})$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $ln\,\Gamma_{K}(\tfrac{n}{2})=\tfrac{K(K-1)}{4}ln\,\pi+\sum_{i:1}^{K}ln\,\Gamma(\tfrac{n-i+1}{2})$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathbb{E}_{q}[ln\,p(\Lambda)] & = & -\tfrac{K(K+1)}{2}ln\,2+\tfrac{n-K-1}{2}\psi_{K}(\tfrac{v}{2})-ln\,\Gamma_{K}(\tfrac{n}{2})\\
 &  & -\tfrac{v}{2}Tr\,(S^{-1}W)+\tfrac{n-K-1}{2}ln\,|W|-\tfrac{n}{2}ln\,|S|
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
so we have:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\boxed{\mathbb{E}_{q}[ln\,p(\Lambda)]=-\tfrac{K(K+1)}{2}ln\,2+\tfrac{n-K-1}{2}\psi_{K}(\tfrac{v}{2})-ln\,\Gamma_{K}(\tfrac{n}{2})-\tfrac{v}{2}Tr\,(S^{-1}W)+\tfrac{n-K-1}{2}ln\,|W|-\tfrac{n}{2}ln\,|S|}$
\end_inset


\end_layout

\begin_layout Standard
\noindent
or 
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\boxed{\mathbb{E}_{q}[ln\,p(\Lambda)]=-\tfrac{K(K+1)}{2}ln\,2+\tfrac{n-K-1}{2}\psi_{K}(\tfrac{v}{2})-ln\,\Gamma_{K}(\tfrac{n}{2})-\tfrac{v}{2}Tr\,(S^{-1}W)-\tfrac{K+1}{2}ln\,|W|+\tfrac{n}{2}ln\,|S^{-1}W|}$
\end_inset


\end_layout

\begin_layout Subsection
\noindent
Two Betas
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\beta\sim q\sim Beta(b)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\beta\sim p\sim Beta(\eta)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathbb{E}_{q}[ln\,p(\beta)] & = & \mathbb{E}_{q}\Big[ln\,\Gamma(\eta_{0}+\eta_{1})-ln\,\Gamma(\eta_{0})-ln\,\Gamma(\eta_{1})+(\eta_{0}-1)ln\,\beta+(\eta_{1}-1)ln\,(1-\beta)\Big]\\
 & = & ln\,\Gamma(\eta_{0}+\eta_{1})-ln\,\Gamma(\eta_{0})-ln\,\Gamma(\eta_{1})+(\eta_{0}-1)\big(\psi(b_{0})-\psi(b_{0}+b_{1})\big)+\\
 &  & (\eta_{1}-1)\big(\psi(b_{1})-\psi(b_{0}+b_{1})\big)\\
 & = & ln\,\Gamma(\eta_{0}+\eta_{1})-ln\,\Gamma(\eta_{0})-ln\,\Gamma(\eta_{1})+(\eta_{0}-1)\psi(b_{0})+\\
 &  & (\eta_{1}-1)\psi(b_{1})-(\eta_{0}+\eta_{1}-2)\psi(b_{0}+b_{1})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that 
\begin_inset Formula $\mathbb{E}_{q}[ln\,\beta]=\psi(b_{0})-\psi(b_{0}+b_{1})$
\end_inset


\end_layout

\begin_layout Standard
\noindent
so :
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{q}[ln\,p(\beta)] & =ln\,\Gamma(\eta_{0}+\eta_{1})-ln\,\Gamma(\eta_{0})-ln\,\Gamma(\eta_{1})+(\eta_{0}-1)\psi(b_{0})+\\
 & (\eta_{1}-1)\psi(b_{1})-(\eta_{0}+\eta_{1}-2)\psi(b_{0}+b_{1})\\
\end{align*}

\end_inset


\end_layout

\begin_layout Section
\noindent
Entropies
\end_layout

\begin_layout Subsection
\noindent
Normal
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $q(x)\sim\mathcal{N}(m,M)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\boxed{H[q]=\tfrac{K}{2}ln\,(2\pi)+\tfrac{K}{2}-\tfrac{1}{2}ln\,|M|}$
\end_inset


\end_layout

\begin_layout Subsection
\noindent
Wishart
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\Lambda\sim q\sim\mathcal{W}(v,W)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
H[q] & = & -\tfrac{v-K-1}{2}\mathbb{E}_{q}ln|\Lambda|-(-\tfrac{1}{2}\mathbb{E}_{q}Tr\,(W^{-1}\Lambda))+\tfrac{v}{2}ln\,|W|+\tfrac{vK}{2}ln\,2+ln\,\Gamma_{K}(\tfrac{v}{2})\\
 & = & -\tfrac{v-K-1}{2}(\psi_{K}(\tfrac{v}{2})+\tfrac{Kv}{2}+Kln\,2+ln\,|W|)+\tfrac{v}{2}ln\,|W|+\tfrac{vK}{2}ln\,2+ln\,\Gamma_{K}(\tfrac{v}{2})\\
 & = & \tfrac{K(K+1)}{2}ln\,2+\tfrac{K+1}{2}ln\,|W|-\tfrac{v-K-1}{2}\psi_{p}(\tfrac{v}{2})+ln\,\Gamma_{K}(\tfrac{v}{2})+\tfrac{Kv}{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
so 
\end_layout

\begin_layout Standard
\noindent

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $\boxed{H[q]=\tfrac{K(K+1)}{2}ln\,2+\tfrac{K+1}{2}ln\,|W|-\tfrac{v-K-1}{2}\psi_{K}(\tfrac{v}{2})+ln\,\Gamma_{K}(\tfrac{v}{2})+\tfrac{Kv}{2}}$
\end_inset


\end_layout

\begin_layout Subsection
\noindent
Beta
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\beta\sim q\sim Beta(b)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
H[q] & = & ln\,\Gamma(b_{0})+ln\,\Gamma(b_{1})-ln\,\Gamma(b_{0}+b_{1})-(b_{0}-1)\mathbb{E}_{q}[ln\,\beta]-(b_{1}-1)\mathbb{E}_{q}[ln\,(1-\beta)]\\
 & = & ln\,\Gamma(b_{0})+ln\,\Gamma(b_{1})-ln\,\Gamma(b_{0}+b_{1})-(b_{0}-1)\psi(b_{0})-(b_{1}-1)\psi(b_{1})+(b_{0}+b_{1}-2)\psi(b_{0}+b_{1})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
So,
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\boxed{H[q]=ln\,\Gamma(b_{0})+ln\,\Gamma(b_{1})-ln\,\Gamma(b_{0}+b_{1})-(b_{0}-1)\psi(b_{0})-(b_{1}-1)\psi(b_{1})+(b_{0}+b_{1}-2)\psi(b_{0}+b_{1})}$
\end_inset


\end_layout

\begin_layout Subsection
\noindent
Multinomial(,1) or Categorical
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $z\sim q\sim Cat(\phi)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
H[q] & = & -\sum_{k}\mathbb{E}_{q}[z_{k}]ln\,\phi_{k}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
so,
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\boxed{H[q]=-\sum_{k}\phi_{k}ln\,\phi_{k}}$
\end_inset


\end_layout

\begin_layout Section
\noindent
Variational ELBO
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mathcal{L}=\mathbb{E}_{q}\Big[ln\,p(joint)\Big]+H_{q}[params]$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
ln\,p(joint) & = & ln\,p(\mu|m_{0},M_{0})+ln\,p(\Lambda|\ell_{0},L_{0})+\sum_{a}ln\,p(\theta_{a}|\mu,\Lambda)+\sum_{a}\sum_{b}ln\,p(z_{a\rightarrow b}|\theta_{a})\\
 &  & +\sum_{a}\sum_{b}ln\,p(z_{a\leftarrow b}|\theta_{b})+\sum_{k}ln\,p(\beta_{kk}|\eta)+\sum_{a}\sum_{b}ln\,p(y_{ab}|z_{a\rightarrow b},z_{a\leftarrow b},\beta)\\
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
H_{q}[params] & = & H_{q}[\mu]+H_{q}[\Lambda]+H_{q}[\theta]+H_{q}[\beta]+H_{q}[z_{\rightarrow}]+H_{q}[z_{\leftarrow}]\\
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Furthermore,
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathbb{E}_{q}\Big[ln\,p(joint)\Big] & =\mathbb{E}_{q}[ & ln\,p(\mu|m_{0},M_{0})]+\mathbb{E}_{q}[ln\,p(\Lambda|\ell_{0},L_{0})]+\sum_{a}\mathbb{E}_{q}[ln\,p(\theta_{a}|\mu,\Lambda)]+\\
 &  & \sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,p(z_{a\rightarrow b}|\theta_{a})]+\sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,p(z_{a\leftarrow b}|\theta_{b})]+\\
 &  & \sum_{k}\mathbb{E}_{q}[ln\,p(\beta_{kk}|\eta)]+\sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,p(y_{ab}|z_{a\rightarrow b},z_{a\leftarrow b},\beta)]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
We parametrize the variational distribution as follows:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mu & \sim & q(\mu|m,M)\sim\mathcal{N}(\mu|m,M)\\
\Lambda & \sim & q(\Lambda|\ell,L)\sim\mathcal{W}(\Lambda|\ell,L)\\
\theta_{a} & \sim & q(\theta_{a}|\mu_{a},\Lambda_{a})\sim\mathcal{N}(\theta_{a}|\mu_{a},\Lambda_{a})\\
\beta_{kk} & \sim & q(\beta_{kk}|b_{k})\sim\mathcal{B}(b_{k0},b_{k1})\\
z_{a\rightarrow b} & \sim & q(z_{a\rightarrow b}|\phi_{a\rightarrow b})\sim Cat(z_{a\rightarrow b}|\phi_{a\rightarrow b})\\
z_{a\leftarrow b} & \sim & q(z_{a\leftarrow b}|\phi_{a\leftarrow b})\sim Cat(z_{a\leftarrow b}|\phi_{a\leftarrow b})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Using the results from above regarding the negative cross entropies:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathbb{E}_{q}\Big[ln\,p(joint)\Big] & = & -\tfrac{K}{2}ln\,2\pi+\tfrac{1}{2}ln\,|M_{0}|-\tfrac{1}{2}\Big(Tr\,M_{0}\Big[M^{-1}+(m-m_{0})(m-m_{0})^{T}\Big]\Big)\\
 &  & -\tfrac{K(K+1)}{2}ln\,2+\tfrac{\ell_{0}-K-1}{2}\psi_{K}(\tfrac{\ell}{2})-ln\,\Gamma_{K}(\tfrac{\ell_{0}}{2})-\tfrac{\ell}{2}Tr\,(L_{0}^{-1}L)-\tfrac{K+1}{2}ln\,|L|\\
 &  & +\tfrac{\ell_{0}}{2}ln\,|L_{0}^{-1}L|-\sum_{a}\tfrac{K}{2}ln\,2\pi+\tfrac{1}{2}\sum_{a}\psi_{K}(\tfrac{\ell}{2})+\tfrac{1}{2}\sum_{a}Kln\,2+\tfrac{1}{2}\sum_{a}ln\,|L|\\
 &  & -\tfrac{\ell}{2}\Big(Tr\,\Big[L\Big(\sum_{a}\big(\Lambda_{a}^{-1}+(m-\mu_{a})(m-\mu_{a})^{T}\big)+\sum_{a}M^{-1}\Big)\Big\}\Big)\\
 &  & +\sum_{a}\sum_{b}\sum_{k}\phi_{a\rightarrow b,k}\mu_{a,k}-\sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,(\sum_{l}exp(\theta_{a,l}))]\\
 &  & +\sum_{a}\sum_{b}\sum_{k}\phi_{a\leftarrow b,k}\mu_{b,k}-\sum_{a}\sum_{b}\mathbb{E}_{q}[ln\,(\sum_{l}exp(\theta_{b,l}))]\\
 &  & +\sum_{k}ln\,\Gamma(\eta_{0}+\eta_{1})-\sum_{k}ln\,\Gamma(\eta_{0})-\sum_{k}ln\,\Gamma(\eta_{1})+\sum_{k}(\eta_{0}-1)\psi(b_{k0})\\
 &  & +\sum_{k}(\eta_{1}-1)\psi(b_{k1})-\sum_{k}(\eta_{0}+\eta_{1}-2)\psi(b_{k0}+b_{k1})\\
 &  & +\sum_{a,b\in link}\sum_{k}\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)+ln\,\epsilon\\
 &  & +\sum_{a,b\notin link}\sum_{k}\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)+ln\,(1-\epsilon)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mathbb{E}_{q}[\Lambda]=\ell L$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mathbb{E}_{q}[ln\,|\Lambda|]=\psi_{K}(\tfrac{\ell}{2})+Kln\,2+ln\,|L|$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
-\sum_{a}\tfrac{K}{2}ln\,2\pi+\sum_{a}\tfrac{1}{2}\mathbb{E}_{q}\Big\{ ln\,|\Lambda|\Big\}\\
-\sum_{a}\tfrac{1}{2}\Big(Tr\,\Big[\mathbb{E}_{q}\Big\{\Lambda\Big\}\Lambda_{a}^{-1}\Big]+\mathbb{E}_{q}\Big\{(\mu_{a}-\mu)^{T}\Lambda(\mu_{a}-\mu)\Big\}\Big)=\\
-\sum_{a}\tfrac{K}{2}ln\,2\pi+\sum_{a}\psi_{K}(\tfrac{\ell}{2})+\sum_{a}Kln\,2+\sum_{a}ln\,|L|\\
-\tfrac{\ell}{2}\Big(Tr\,\Big[L\Big(\sum_{a}\big(\Lambda_{a}^{-1}+(m-\mu_{a})(m-\mu_{a})^{T}\big)\Big)\Big\}\Big)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent

\series bold
\begin_inset Formula $ $
\end_inset


\end_layout

\begin_layout Standard
\noindent
For the expression 
\begin_inset Formula $\mathbb{E}_{q}[ln\,(\sum_{l}exp(\theta_{a,l}))]$
\end_inset

, we use the Jensen's inequality to acquire:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathbb{E}_{q}[ln\,(\sum_{l}exp(\theta_{a,l}))] & \leq & ln\,(\sum_{l}\mathbb{E}_{q}[exp(\theta_{a,l})])\\
 & = & ln\,(\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}diag(\Lambda_{a}^{-1})_{ll}))
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
We can introduce another bound that introduces a new variational parameter
 per individual:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{q}[ln\,(\sum_{l}exp(\theta_{a,l}))] & \leq\zeta_{a}^{-1}\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}diag(\Lambda_{a}^{-1})_{ll})+ln\,\zeta_{a}-1\\
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Moreover, using the entropies from above:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
H_{q}[params] & = & \tfrac{K}{2}ln\,(2\pi)+\tfrac{K}{2}-\tfrac{1}{2}ln\,|M|\\
 &  & +\tfrac{K(K+1)}{2}ln\,2+\tfrac{K+1}{2}ln\,|L|-\tfrac{\ell-K-1}{2}\psi_{K}(\tfrac{\ell}{2})+ln\,\Gamma_{K}(\tfrac{\ell}{2})+\tfrac{K\ell}{2}\\
 &  & +\sum_{a}\tfrac{K}{2}ln\,(2\pi)+\sum_{a}\tfrac{K}{2}-\sum_{a}\tfrac{1}{2}ln\,|\Lambda_{a}|\\
 &  & +\sum_{k}ln\,\Gamma(b_{k0})+\sum_{k}ln\,\Gamma(b_{k1})-\sum_{k}ln\,\Gamma(b_{k0}+b_{k1})-\sum_{k}(b_{k0}-1)\psi(b_{k0})\\
 &  & -\sum_{k}(b_{k1}-1)\psi(b_{k1})+\sum_{k}(b_{k0}+b_{k1}-2)\psi(b_{k0}+b_{k1})\\
 &  & -\sum_{a}\sum_{b}\sum_{k}\phi_{a\rightarrow b,k}ln\,\phi_{a\rightarrow b,k}\\
 &  & -\sum_{a}\sum_{b}\sum_{k}\phi_{a\leftarrow b,k}ln\,\phi_{a\leftarrow b,k}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that here I assume the following for the hyper-parameters:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
m_{0} & =\boldsymbol{0}\\
M_{0} & =10\times\boldsymbol{I}\\
\ell_{0} & =K+2\\
L_{0} & =\dfrac{.1}{\ell_{0}}\boldsymbol{I}\\
\eta_{0} & >1=9\\
\eta_{1} & =1
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Note that here I assume the following for the variational parameters:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
m & =\boldsymbol{0}\\
M & =10\times\boldsymbol{I}\\
\ell & =K+2\\
L & =\dfrac{.1}{\ell_{0}}\boldsymbol{I}\\
b_{0} & >1=9\\
b_{1} & =1
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Finally, we have the following:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L} & = & -\tfrac{1}{2}\Bigg(Kln\,2\pi-ln\,|M_{0}|+tr\,M_{0}(m-m_{0})(m-m_{0})^{T}+tr\,M_{0}M^{-1}\Bigg)\\
 &  & +\tfrac{1}{2}\Bigg(-K(K+1)ln\,2+(\ell_{0}-K-1)\sum_{i}\Psi(\tfrac{\ell-i+1}{2})-\tfrac{K(K-1)}{2}ln\,\pi-2\sum_{i}ln\,\Gamma(\tfrac{\ell_{0}-i+1}{2})\\
 &  & -\ell tr\,(L_{0}^{-1}L)-(K+1)ln\,|L|+\ell_{0}ln\,|L_{0}^{-1}L|\Bigg)\\
 &  & -\tfrac{1}{2}\sum_{a}\Bigg(Kln\,2\pi-\sum_{i}\Psi(\tfrac{\ell-i+1}{2})-Kln\,2-ln\,|L|+\\
 &  & \ell tr\,\Big\{ L\big[(\mu_{a}-m)(\mu_{a}-m)^{T}+M^{-1}+\Lambda_{a}^{-1}\big]\Big\}\Bigg)\\
 &  & +\sum_{a}\sum_{b\in sink(a)}\Big(\sum_{k}\phi_{a\rightarrow b,k}\mu_{a,k}-ln\,\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\Lambda_{a,l}^{-1})\Big)\\
 &  & +\sum_{a}\sum_{b\notin sink(a)}\Big(\sum_{k}\phi_{a\rightarrow b,k}\mu_{a,k}-ln\,\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\Lambda_{a,l}^{-1})\Big)\\
 &  & +\sum_{a}\sum_{b\in source(a)}\Big(\sum_{k}\phi_{b\leftarrow a,k}\mu_{a,k}-ln\,\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\Lambda_{a,l}^{-1})\Big)\\
 &  & +\sum_{a}\sum_{b\notin source(a)}\Big(\sum_{k}\phi_{b\leftarrow a,k}\mu_{a,k}-ln\,\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\Lambda_{a,l}^{-1})\Big)\\
 &  & +\sum_{k}ln\,\Gamma(\eta_{0}+\eta_{1})-\sum_{k}ln\,\Gamma(\eta_{0})-\sum_{k}ln\,\Gamma(\eta_{1})+\sum_{k}(\eta_{0}-1)\Psi(b_{k0})+\sum_{k}(\eta_{1}-1)\Psi(b_{k1})\\
 &  & -\sum_{k}(\eta_{0}+\eta_{1}-2)\Psi(b_{k0}+b_{k1})\\
 &  & +\sum_{a}\sum_{b\in sink(a)}\sum_{k}\Big(\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}(\Psi(b_{k0})-\Psi(b_{k0}+b_{k1})-ln\,\epsilon)+ln\,\epsilon\Big)\\
 &  & +\sum_{a}\sum_{b\notin sink(a)}\sum_{k}\Big(\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}(\Psi(b_{k1})-\Psi(b_{k0}+b_{k1})-ln\,(1-\epsilon))+ln\,(1-\epsilon)\Big)\\
 &  & +\tfrac{1}{2}\Big(Kln\,2\pi+K-ln\,|M|\Big)\\
 &  & +\tfrac{1}{2}\Big((K+1)ln\,|L|+K(K+1)ln\,2+\ell K+\tfrac{1}{2}K(K-1)ln\,\pi\\
 &  & +2\sum_{i}ln\,\Gamma(\tfrac{\ell-i+1}{2})-(\ell-K-1)\sum_{i}\Psi(\tfrac{\ell-i+1}{2})\Big)\\
 &  & +\tfrac{1}{2}\sum_{a}\Big(Kln\,2\pi-ln\,|\Lambda_{a}|+K\Big)\\
 &  & +\sum_{k}\Big(ln\,\Gamma(b_{k0})+ln\,\Gamma(b_{k1})-ln\,\Gamma(b_{k0}+b_{k1})-(b_{k0}-1)\Psi(b_{k0})-\\
 &  & (b_{k1}-1)\Psi(b_{k1})+(b_{k0}+b_{k1}-2)\Psi(b_{k0}+b_{k1})\Big)\\
 &  & -\sum_{a}\sum_{b\in sink(a)}\sum_{k}\Big(\phi_{a\rightarrow b,k}ln\,\phi_{a\rightarrow b,k}\Big)\\
 &  & -\sum_{a}\sum_{b\notin sink(a)}\sum_{k}\Big(\phi_{a\rightarrow b,k}ln\,\phi_{a\rightarrow b,k}\Big)\\
 &  & -\sum_{a}\sum_{b\in sink(a)}\sum_{k}\Big(\phi_{a\leftarrow b,k}ln\,\phi_{a\leftarrow b,k}\Big)\\
 &  & -\sum_{a}\sum_{b\notin sink(a)}\sum_{k}\Big(\phi_{a\leftarrow b,k}ln\,\phi_{a\leftarrow b,k}\Big)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
\noindent
ELBO Gradients
\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $m$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{m} & = & -\tfrac{1}{2}\Big(Tr\,M_{0}(m-m_{0})(m-m_{0}){}^{T}\Big]\Big)\\
 &  & -\tfrac{\ell}{2}\Big(Tr\,L\Big(\sum_{a}(\mu_{a}-m)(\mu_{a}-m)^{T}\Big)\Big)\\
 & \propto & Tr\,M_{0}(m-m_{0})(m-m_{0}){}^{T}\\
 &  & +\ell\Big(Tr\,L\big(\sum_{a}mm^{T}+\mu_{a}\mu_{a}^{T}-m\mu_{a}^{T}-\mu_{a}m^{T}\big)\Big)\\
 & =\\
 & \Longrightarrow\\
\nabla_{m}\mathcal{L}_{m} & \propto & 2M_{0}(m-m_{0})-2\ell L\sum_{a}(\mu_{a}-m)=0\\
 & \Longrightarrow\\
 &  & \boxed{m=(M_{0}+N\ell L)^{-1}(M_{0}m_{0}+\ell L\sum_{a}\mu_{a})}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In minibatch node sampling this would be
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
\boxed{m=M^{-1}(M_{0}m_{0}+\ell L\dfrac{N}{\#mbnodes}\sum_{a\in mbnodes}\mu_{a})}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $M$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{M} & = & -\tfrac{1}{2}\Big(Tr\,M_{0}M^{-1}\Big)\\
 &  & -\tfrac{\ell}{2}Tr\,NLM^{-1}\\
 &  & -\tfrac{1}{2}ln\,|M|\\
 & \propto & Tr\,M_{0}M^{-1}+\ell Tr\,NLM^{-1}+ln\,|M|\\
 & \Longrightarrow\\
\nabla_{M^{-1}}\mathcal{L}_{M} & = & 0\\
\\
 & = & -M_{0}-N\ell L+M=0\\
 &  & \boxed{M=M_{0}+N\ell L}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $L$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{L} & = & -\tfrac{\ell}{2}Tr\,(L_{0}^{-1}L)-\tfrac{K+1}{2}ln\,|L|+\tfrac{\ell_{0}}{2}ln\,|L_{0}^{-1}L|\\
 &  & +\tfrac{1}{2}\sum_{a}ln\,|L|-\tfrac{\ell}{2}\Big(Tr\,\Big[L\Big(\sum_{a}\big(\Lambda_{a}^{-1}+(\mu_{a}-m)(\mu_{a}-m)^{T}\big)+\sum_{a}M^{-1}\Big)\Big\}\Big)\\
 &  & +\tfrac{K+1}{2}ln\,|L|\\
 & \propto & -\ell Tr\,(L_{0}^{-1}L)\bcancel{-(K+1)ln\,|L}|+\ell_{0}ln\,|L_{0}^{-1}L|\\
 &  & +\sum_{a}ln\,|L|-\ell\Big(Tr\,\Big[L\Big(\sum_{a}\big(\Lambda_{a}^{-1}+(\mu_{a}-m)(\mu_{a}-m)^{T}\big)+\sum_{a}M^{-1}\Big)\Big\}\Big)\\
 &  & +\bcancel{(K+1)ln\,|L|}\\
 & \Longrightarrow\\
\nabla_{L}\mathcal{L}_{L} & = & -\ell L_{0}^{-1}+\tfrac{1}{2}(\ell_{0}+N)L^{-1}-\ell\Big(\sum_{a}\big(\Lambda_{a}^{-1}+(\mu_{a}-m)(\mu_{a}-m)^{T}\big)+\sum_{a}M^{-1}\Big)^{T}=0\\
 &  & \ell(L_{0}^{-1}+\sum_{a}\Lambda_{a}^{-1}+\sum_{a}(\mu_{a}-m)(\mu_{a}-m)^{T}+NM^{-1})=(N+\ell_{0})L^{-1}\\
 & \Longrightarrow & \boxed{L=\dfrac{(N+\ell_{0})}{\ell}\Bigg((L_{0}^{-1}+\sum_{a}\big(\Lambda_{a}^{-1}+(\mu_{a}-m)(\mu_{a}-m)^{T}\big)+\sum_{a}M^{-1})\Bigg)^{-1}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
optimizing simultaneously with 
\begin_inset Formula $\ell$
\end_inset

 in the mini-batch setting:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
\boxed{L=\Bigg((L_{0}^{-1}+\dfrac{N}{\#mbnodes}\big\{\sum_{a}\Lambda_{a}^{-1}+\sum_{a}(\mu_{a}-m)(\mu_{a}-m)^{T}\big\}+NM^{-1})\Bigg)^{-1}}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $\ell$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\ell} & = & revise\\
 & \propto\\
 & \Longrightarrow\\
\nabla_{\ell}\mathcal{L}_{\ell} & =\\
 & \Longrightarrow\\
 & hence,\\
 & \Longrightarrow\\
 &  & \boxed{\ell=\ell_{0}+N}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $b_{k}$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{b_{k}} & = & revise\\
 &  & \text{simultaenously optimizing \ensuremath{b_{k0}}},\mbox{\ensuremath{b_{k1}}}\\
 & \Longrightarrow & \text{Similar to our previous results}\\
\nabla_{b_{k0}}\mathcal{L}_{b_{k}} & = & 0\\
 & \Longrightarrow & \boxed{b_{k0}=\eta_{0}+\dfrac{\#trainlinks}{\#mblinks}\sum_{a,b\in mblinks}\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}}\\
\nabla_{b_{k1}}\mathcal{L}_{b_{k}} & = & 0\\
 &  & \boxed{b_{k1}=\eta_{1}+\dfrac{\#trainnonlinks}{\#mbnonlinks}\sum_{a,b\notin mblinks}\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $\phi_{a\rightarrow b,k}$
\end_inset

 for links
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\phi_{a\rightarrow b,k}} & = & \phi_{a\rightarrow b,k}\mu_{a,k}\\
 &  & +\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)\\
 &  & -\phi_{a\rightarrow b,k}ln\,\phi_{a\rightarrow b,k}\\
 & = & \phi_{a\rightarrow b,k}\Big(\mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)-ln\,\phi_{a\rightarrow b,k}\Big)\\
\nabla_{\phi_{a\rightarrow b,k}}\mathcal{L}_{\phi_{a\rightarrow b,k}} & = & \mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)-ln\,\phi_{a\rightarrow b,k}=0\\
 &  & \boxed{\phi_{a\rightarrow b,k}\propto exp\Bigg\{\mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)\Bigg\}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $\phi_{a\leftarrow b,k}$
\end_inset

 for links
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\phi_{a\leftarrow b,k}} & = & \phi_{a\leftarrow b,k}\mu_{b,k}\\
 &  & +\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)\\
 &  & -\phi_{a\leftarrow b,k}ln\,\phi_{a\leftarrow b,k}\\
 & = & \phi_{a\leftarrow b,k}\Big(\mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)-ln\,\phi_{a\leftarrow b,k}\Big)\\
\nabla_{\phi_{a\leftarrow b,k}}\mathcal{L}_{\phi_{a\leftarrow b,k}} & = & \mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)-ln\,\phi_{a\leftarrow b,k}=0\\
 &  & \boxed{\phi_{a\leftarrow b,k}\propto exp\Bigg\{\mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k0})-\psi(b_{k0}+b_{k1})-ln\,\epsilon\big)\Bigg\}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $\phi_{a\rightarrow b,k}$
\end_inset

 for non-links
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\phi_{a\rightarrow b,k}} & = & \phi_{a\rightarrow b,k}\mu_{a,k}\\
 &  & +\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)\\
 &  & -\phi_{a\rightarrow b,k}ln\,\phi_{a\rightarrow b,k}\\
 & = & \phi_{a\rightarrow b,k}\Big(\mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)-ln\,\phi_{a\rightarrow b,k}\Big)\\
\nabla_{\phi_{a\rightarrow b,k}}\mathcal{L}_{\phi_{a\rightarrow b,k}} & = & \mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)-ln\,\phi_{a\rightarrow b,k}=0\\
 &  & \boxed{\phi_{a\rightarrow b,k}\propto exp\Bigg\{\mu_{a,k}+\phi_{a\leftarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)\Bigg\}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $\phi_{a\leftarrow b,k}$
\end_inset

 for non-links
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\phi_{a\leftarrow b,k}} & = & \phi_{a\leftarrow b,k}\mu_{b,k}\\
 &  & +\phi_{a\rightarrow b,k}\phi_{a\leftarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)\\
 &  & -\phi_{a\leftarrow b,k}ln\,\phi_{a\leftarrow b,k}\\
 & = & \phi_{a\leftarrow b,k}\Big(\mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)-ln\,\phi_{a\leftarrow b,k}\Big)\\
\nabla_{\phi_{a\leftarrow b,k}}\mathcal{L}_{\phi_{a\leftarrow b,k}} & = & \mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)-ln\,\phi_{a\leftarrow b,k}=0\\
 &  & \boxed{\phi_{a\leftarrow b,k}\propto exp\Bigg\{\mu_{b,k}+\phi_{a\rightarrow b,k}\big(\psi(b_{k1})-\psi(b_{k0}+b_{k1})-ln\,(1-\epsilon)\big)\Bigg\}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $\mu_{a}$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mu_{a}$
\end_inset

 and 
\begin_inset Formula $\Lambda_{a}$
\end_inset

 are two of the scarier ones.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}_{\mu_{a}} & = & -\tfrac{\ell}{2}\big[(\mu_{a}-m)^{T}L(\mu_{a}-m)\big]+\\
 &  & \sum_{b\in sink(a)}\phi_{a\rightarrow b}^{T}\mu_{a}+\\
 &  & \sum_{b\notin sink(a)}\phi_{a\rightarrow b}^{T}\mu_{a}+\\
 &  & \sum_{b\in source(a)}\phi_{b\leftarrow a}^{T}\mu_{a}+\\
 &  & \sum_{b\notin source(a)}\phi_{b\leftarrow a}^{T}\mu_{a}-\\
 &  & \sum_{b}log\,\Big(\boldsymbol{1}^{T}\underbar{f}(\mu_{a},\Lambda_{a})\Big)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
where 
\begin_inset Formula $\underbar{f}(\mu_{a},\Lambda_{a})=\left(\begin{array}{c}
exp(\mu_{a,1}+\tfrac{1}{2}\Lambda_{a,1}^{-1})\\
\vdots\\
exp(\mu_{a,k}+\tfrac{1}{2}\Lambda_{a,k}^{-1})\\
\vdots\\
exp(\mu_{a,K}+\tfrac{1}{2}\Lambda_{a,K}^{-1})
\end{array}\right)$
\end_inset

, and we may for convenience interchangeably use 
\begin_inset Formula $\underbar{f}_{a}$
\end_inset

 to refer to 
\begin_inset Formula $\underbar{f}(\mu_{a},\Lambda_{a}):$
\end_inset


\end_layout

\begin_layout Standard
\noindent
Hence the geradient is 
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
\nabla_{\mu_{a}}\mathcal{L}_{\mu_{a}} & = & -\ell L(\mu_{a}-m)+\\
 &  & \sum_{b\in sink(a)}\phi_{a\rightarrow b}+\sum_{b\notin sink(a)}\phi_{a\rightarrow b}+\\
 &  & \sum_{b\in source(a)}\phi_{b\leftarrow a}+\sum_{b\notin source(a)}\phi_{b\leftarrow a}-\\
 &  & \sum_{b}\dfrac{\partial\underbar{f}(\mu_{a},\Lambda_{a})}{\partial\mu_{a}}(\boldsymbol{1})\\
 & = & -\ell L(\mu_{a}-m)+\\
 &  & \sum_{b\in sink(a)}\phi_{a\rightarrow b}+\sum_{b\notin sink(a)}\phi_{a\rightarrow b}+\\
 &  & \sum_{b\in source(a)}\phi_{b\leftarrow a}+\sum_{b\notin source(a)}\phi_{b\leftarrow a}-\\
 &  & \sum_{b}\dfrac{\boldsymbol{J}_{\underbar{f}}\times\boldsymbol{1}}{\boldsymbol{1}^{T}\underbar{f}(\mu_{a},\Lambda_{a})}\\
 & = & -\ell L(\mu_{a}-m)+\\
 &  & \sum_{b\in sink(a)}\phi_{a\rightarrow b}+\sum_{b\notin sink(a)}\phi_{a\rightarrow b}+\\
 &  & \sum_{b\in source(a)}\phi_{b\leftarrow a}+\sum_{b\notin source(a)}\phi_{b\leftarrow a}-\\
 &  & \sum_{b}\dfrac{\left(\begin{array}{ccccc}
\dfrac{\partial\underbar{f}_{a1}}{\partial\mu_{a1}} & \ldots & \dfrac{\partial\underbar{f}_{a1}}{\partial\mu_{ak}} & \ldots & \dfrac{\partial\underbar{f}_{a1}}{\partial\mu_{aK}}\\
\vdots & \ddots &  &  & \vdots\\
\dfrac{\partial\underbar{f}_{ak}}{\partial\mu_{a1}} & \ldots & \dfrac{\partial\underbar{f}_{ak}}{\partial\mu_{ak}} & \ldots & \dfrac{\partial\underbar{f}_{ak}}{\partial\mu_{aK}}\\
\vdots &  &  & \ddots & \vdots\\
\dfrac{\partial\underbar{f}_{aK}}{\partial\mu_{a1}} &  & \ldots &  & \dfrac{\partial\underbar{f}_{aK}}{\partial\mu_{aK}}
\end{array}\right)}{\boldsymbol{1}^{T}\underbar{f}(\mu_{a},\Lambda_{a})}\\
 & = & -\ell L(\mu_{a}-m)+\\
 &  & \sum_{b\in sink(a)}\phi_{a\rightarrow b}+\sum_{b\notin sink(a)}\phi_{a\rightarrow b}+\\
 &  & \sum_{b\in source(a)}\phi_{b\leftarrow a}+\sum_{b\notin source(a)}\phi_{b\leftarrow a}-\\
 &  & \sum_{b}\underbar{sfx}(a)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
where 
\begin_inset Formula $\underbar{sfx}(a)=\left(\begin{array}{c}
\dfrac{exp(\mu_{a,1}+\tfrac{1}{2}\:\Lambda_{a,1}^{-1})}{\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\:\Lambda_{a,l}^{-1})}\\
\vdots\\
\dfrac{exp(\mu_{a,k}+\tfrac{1}{2}\:\Lambda_{a,k}^{-1})}{\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\:\Lambda_{a,l}^{-1})}\\
\vdots\\
\dfrac{exp(\mu_{a,1}+\tfrac{1}{2}\:\Lambda_{a,1}^{-1})}{\sum_{l}exp(\mu_{a,l}+\tfrac{1}{2}\:\Lambda_{a,l}^{-1})}
\end{array}\right)$
\end_inset


\end_layout

\begin_layout Standard
\noindent
so all in all the gradient is :
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\nabla_{\mu_{a}}\mathcal{L}_{\mu_{a}}=$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\boxed{-\ell L(\mu_{a}-m)+\sum_{b\in sink(a)}\phi_{a\rightarrow b}+\sum_{b\notin sink(a)}\phi_{a\rightarrow b}+\sum_{b\in source(a)}\phi_{b\leftarrow a}+\sum_{b\notin source(a)}\phi_{b\leftarrow a}-\sum_{b}\underbar{sfx}(a)}$
\end_inset


\end_layout

\begin_layout Standard
\noindent
Similarly the Hessian will be as follows:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
\nabla_{\mu_{a}}^{2}\mathcal{L}_{\mu_{a}} & = & -\ell L-\\
 &  & \sum_{b}\dfrac{\partial\underbar{sfx}(a)}{\partial\mu_{a}^{T}}\\
 & = & \ell L-\\
 &  & \sum_{b}\boldsymbol{J}_{\underbar{sfx}(a)}\\
 & = & -\ell L-\\
 &  & \sum_{b}\left(\begin{array}{ccccc}
\dfrac{\partial\underbar{sfx}_{a1}}{\partial\mu_{a1}} & \ldots & \dfrac{\partial\underbar{sfx}_{a1}}{\partial\mu_{ak}} & \ldots & \dfrac{\partial\underbar{sfx}_{a1}}{\partial\mu_{aK}}\\
\vdots & \ddots &  &  & \vdots\\
\dfrac{\partial\underbar{sfx}_{ak}}{\partial\mu_{a1}} & \ldots & \dfrac{\partial\underbar{sfx}_{ak}}{\partial\mu_{ak}} & \ldots & \dfrac{\partial\underbar{sfx}_{ak}}{\partial\mu_{aK}}\\
\vdots &  &  & \ddots & \vdots\\
\dfrac{\partial s\underbar{fx}_{aK}}{\partial\mu_{a1}} &  & \ldots &  & \dfrac{\partial\underbar{sfx}_{aK}}{\partial\mu_{aK}}
\end{array}\right)\\
 & = & -\ell L-\\
 &  & \sum_{b}\left(\begin{array}{ccccc}
\underbar{sfx}_{a1}-\underbar{sfx}_{a1}^{2} & \ldots & -\underbar{sfx}_{a1}\underbar{sfx}_{ak} & \ldots & \underbar{-sfx}_{a1}\underbar{sfx}_{aK}\\
\vdots & \ddots &  &  & \vdots\\
\underbar{-sfx}_{a1}\underbar{sfx}_{ak} & \ldots & \underbar{sfx}_{ak}-\underbar{sfx}_{ak}^{2} & \ldots & \underbar{-sfx}_{ak}\underbar{sfx}_{ak}\\
\vdots &  &  & \ddots & \vdots\\
-\underbar{sfx}_{a1}\underbar{sfx}_{aK} &  & \ldots &  & \underbar{-sfx}_{aK}-\underbar{sfx}_{aK}^{2}
\end{array}\right)\\
 & = & \boxed{-\ell L-\sum_{b}\Big(diagm(\underbar{sfx}_{a})-\underbar{sfx}_{a}\underbar{sfx}_{a}^{T}\Big)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
The newton step would look like:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\mu_{a,k}=\mu_{a,k}-H_{\mu_{a,k}}^{-1}G_{\mu_{a,k}}$
\end_inset


\end_layout

\begin_layout Subsection
\noindent
Gradient with respect to 
\begin_inset Formula $\Lambda_{a}$
\end_inset


\end_layout

\begin_layout Standard
\noindent
similarly assuming that 
\begin_inset Formula $\Lambda_{a}$
\end_inset

 is a diagonal matrix(or a column vector).
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula 
\begin{align*}
\mathcal{L}_{\Lambda_{a}^{-1}} & = & -\tfrac{\ell}{2}diag\,(L)'\Lambda_{a}^{-1}+\tfrac{1}{2}ln\,|diagm(\Lambda_{a}^{-1})|-\sum_{b}log\,\Big(\boldsymbol{1}^{T}\underbar{f}(\mu_{a},\Lambda_{a})\Big)\\
 & =\\
\nabla_{\Lambda_{a}^{-1}}\mathcal{L}_{\Lambda_{a}^{-1}}=G_{\Lambda_{a}^{-1}} & = & \boxed{-\tfrac{\ell}{2}diag(L)+\tfrac{1}{2}(\Lambda_{a})-\tfrac{1}{2}\sum_{b}(\underbar{sfx}(a))}\\
 &  & \boxed{}\\
\nabla_{\Lambda_{a}^{-1}}^{2}\mathcal{L}_{\Lambda_{a}^{-1}}=H_{\Lambda_{a}^{-1}} & \propto & \boxed{-\tfrac{1}{2}diagm(\Lambda_{a}\odot\Lambda_{a})-\tfrac{1}{4}\sum_{b}\Big(diagm(\underbar{sfx}_{a})-\underbar{sfx}_{a}\underbar{sfx}_{a}^{T}\Big)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\noindent
The newton step would look like:
\end_layout

\begin_layout Standard
\noindent
\begin_inset Formula $\Lambda_{a}^{-1}=\Lambda_{a}^{-1}-H_{\Lambda_{a}^{-1}}^{-1}G_{\Lambda_{a}^{-1}}$
\end_inset


\end_layout

\end_body
\end_document
